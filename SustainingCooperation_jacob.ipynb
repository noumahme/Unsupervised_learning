{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Tables\n",
    "\n",
    "| Partner Selection  | LEAVE | STAY |\n",
    "| :----------------- | :---: | :--: |\n",
    "| PARTNER_DEFECTED   | 0     | 0    |\n",
    "| PARTNER_COOPERATED | 0     | 0    |\n",
    "\n",
    "`LEAVE` and `STAY` are the two actions in the partner selection stage.  \n",
    "If two agents are paired and one agent chooses to leave then both agents will be added to a pool. \n",
    "After all agents have made their choices agents in the pool are randomly paired. \n",
    "If both agents choose to stay then they will remain paired for the prisoner's dilemma stage.\n",
    "\n",
    "| Prisoner's Dilemma  | DEFECT | COOPERATE |\n",
    "| :------------------ | :----: | :-------: |\n",
    "| PARTNER_DEFECTED    | 0      | 0         |\n",
    "| PARTNER_COOPERATED  | 0      | 0         |\n",
    "\n",
    "`DEFECT` and `COOPERATE` are the two actions in the prisoner's dilemma stage.\n",
    "Agents are given rewards in the prisoner's dilemma stage which can be seen in the table below \n",
    "where ($r_i$, $r_j$) is the returned rewards for agents `i` and `j`\n",
    "\n",
    "|                    | `j` Defects | `j` Cooperates |\n",
    "| ------------------ | ----------- | -------------- |\n",
    "| **`i` Defects**    | (1, 1)      | (5, 0)         |\n",
    "| **`i` Cooperates** | (0, 5)      | (3, 3)         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ActionPS(Enum):\n",
    "    LEAVE = 0\n",
    "    STAY = 1\n",
    "\n",
    "class ActionPD(Enum):\n",
    "    DEFECT = 0\n",
    "    COOPERATE = 1\n",
    "\n",
    "class State(Enum):\n",
    "    PARTNER_DEFECTED = 0\n",
    "    PARTNER_COOPERATED = 1\n",
    "\n",
    "def get_state(action: ActionPD) -> State:\n",
    "    return State.PARTNER_COOPERATED if action == ActionPD.COOPERATE else State.PARTNER_DEFECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyPS(Enum):\n",
    "    ALWAYS_STAY = 0\n",
    "    OUT_FOR_TAT = 1\n",
    "    REVERSE_OUT_FOR_TAT = 2\n",
    "    ALWAYS_LEAVE = 3\n",
    "    RANDOM = 4\n",
    "\n",
    "class StrategyPD(Enum):\n",
    "    ALWAYS_COOPERATE = 0\n",
    "    TIT_FOR_TAT = 1\n",
    "    REVERSE_TIT_FOR_TAT = 2\n",
    "    ALWAYS_DEFECT = 3\n",
    "    RANDOM = 4\n",
    "    \n",
    "strategy_names = {\n",
    "    StrategyPS.ALWAYS_STAY : 'Always-Stay',\n",
    "    StrategyPS.OUT_FOR_TAT : 'Out-For-Tat',\n",
    "    StrategyPS.REVERSE_OUT_FOR_TAT : 'Reverse-OFT',\n",
    "    StrategyPS.ALWAYS_LEAVE : 'Always-Leave',\n",
    "    StrategyPS.RANDOM : 'Random (PS)',\n",
    "    StrategyPD.ALWAYS_COOPERATE : 'Always-Cooperate',\n",
    "    StrategyPD.TIT_FOR_TAT : 'Tit-For-Tat',\n",
    "    StrategyPD.REVERSE_TIT_FOR_TAT : 'Reverse-TFT',\n",
    "    StrategyPD.ALWAYS_DEFECT : 'Always-Defect',\n",
    "    StrategyPD.RANDOM : 'Random (PD)',\n",
    "}\n",
    "\n",
    "strategy_colors = {\n",
    "    StrategyPS.ALWAYS_STAY : 'lightcoral',\n",
    "    StrategyPS.OUT_FOR_TAT : 'lightsteelblue',\n",
    "    StrategyPS.REVERSE_OUT_FOR_TAT : 'lightgreen',\n",
    "    StrategyPS.ALWAYS_LEAVE : 'tan',\n",
    "    StrategyPS.RANDOM : 'mediumpurple',\n",
    "    StrategyPD.ALWAYS_COOPERATE : 'red',\n",
    "    StrategyPD.TIT_FOR_TAT : 'blue',\n",
    "    StrategyPD.REVERSE_TIT_FOR_TAT : 'green',\n",
    "    StrategyPD.ALWAYS_DEFECT : 'yellow',\n",
    "    StrategyPD.RANDOM : 'purple',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the probabilities of selection actions given the current state \n",
    "def boltzmann_exploration(q_table, state: State, temperature: float, constant):\n",
    "    exp = np.exp((q_table[state.value, :] - max(q_table[state.value, :])) / temperature)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "def epsilon_greedy(q_table, state: State, epsilon: float) -> np.ndarray:\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.ones(len(q_table[state.value, :])) / len(q_table[state.value, :])\n",
    "    else:\n",
    "        prob = np.zeros(len(q_table[state.value, :]))\n",
    "        prob[np.argmax(q_table[state.value, :])] = 1\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs the Q-Learning algorithm on the provided qtable\n",
    "# NOTE: alpha is the learning rate and gamma is the discount rate\n",
    "def q_learning(qtable, next_qtable, state: State, action, \n",
    "               reward: float, new_state: State, alpha: float, gamma: float) -> None:\n",
    "    qtable[state.value, action.value] = (1.0 - alpha) * qtable[state.value, action.value] + \\\n",
    "        alpha * (reward + gamma * np.max(next_qtable[new_state.value, :]))\n",
    "    \n",
    "def sarsa_learning(qtable, next_qtable, state: State, action, \n",
    "               reward: float, new_state: State, new_action, alpha: float, gamma: float) -> None:\n",
    "    qtable[state.value, action.value] = (1.0 - alpha) * qtable[state.value, action.value] + \\\n",
    "        alpha * (reward + gamma * next_qtable[new_state.value, new_action.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, learning_rate: float, temperature: float, discount_rate: float, \n",
    "                 delta_t: float,\n",
    "                 last_action_pd: ActionPD = None, qtable_ps = None, qtable_pd = None):\n",
    "        self.a = learning_rate\n",
    "        self.t = temperature\n",
    "        self.g = discount_rate\n",
    "        self.delta_t = delta_t\n",
    "        self.last_trajectory = None\n",
    "        self.last_action_pd = np.random.choice([ActionPD.DEFECT, ActionPD.COOPERATE], 1) if last_action_pd == None else last_action_pd\n",
    "        self.qtable_ps = np.zeros((2, 2)) if qtable_ps is None else qtable_ps\n",
    "        self.qtable_pd = np.zeros((2, 2)) if qtable_pd is None else qtable_pd\n",
    "        \n",
    "        # Force agents to use Out For Tat Partner Selection Strategy\n",
    "        # self.qtable_ps[0, 0] = 10\n",
    "        # self.qtable_ps[1, 1] = 10\n",
    "        \n",
    "    # returns an action given the current state\n",
    "    def get_action_ps(self, state: State, debug = False) -> ActionPS:\n",
    "        temp = boltzmann_exploration(self.qtable_ps, state, self.t, 0.8)\n",
    "        action = np.random.choice([ActionPS.LEAVE, ActionPS.STAY], p=temp)\n",
    "        if debug:\n",
    "            print(\"Action Probabilities: \" + str(temp))\n",
    "            print(\"Chosen Action: \" + str(action))\n",
    "        return action\n",
    "\n",
    "        # Force Out for Tat\n",
    "        # if state == State.PARTNER_COOPERATED:\n",
    "        #     return ActionPS.STAY\n",
    "        # else:\n",
    "        #     return ActionPS.LEAVE\n",
    "\n",
    "    def get_action_pd(self, state: State, debug = False) -> ActionPD:\n",
    "        temp = boltzmann_exploration(self.qtable_pd, state, self.t, 0.8)\n",
    "        action = np.random.choice([ActionPD.DEFECT, ActionPD.COOPERATE], p=temp)\n",
    "        if debug:\n",
    "            print(\"Action Probabilities: \" + str(temp))\n",
    "            print(\"Chosen Action: \" + str(action))\n",
    "        return action\n",
    "    \n",
    "    def update_reward(self, reward):\n",
    "        pass\n",
    "    \n",
    "    # trains using trajectories from each round\n",
    "    def train(self, trajectories, learning_mode, \n",
    "              last_trajectory = None, debug = False):\n",
    "        \n",
    "        # Author's Implementation\n",
    "\n",
    "        # discounted_rewards = []\n",
    "        # running_sum = 0\n",
    "        # for trajectory in trajectories[::-1]:\n",
    "        #     running_sum = trajectory[6] + self.g * running_sum\n",
    "        #     discounted_rewards.append(running_sum)\n",
    "        # discounted_rewards.reverse()\n",
    "\n",
    "        # for idx, trajectory in enumerate(trajectories):\n",
    "        #     # partner selection training\n",
    "        #     q_learning(self.qtable_ps, self.qtable_pd, trajectory[0], trajectory[1], discounted_rewards[idx], trajectory[3], self.a, self.g)\n",
    "        #     # prisoner's dilemma training\n",
    "        #     q_learning(self.qtable_pd, self.qtable_ps, trajectory[3], trajectory[4], discounted_rewards[idx], trajectory[5], self.a, self.g)\n",
    "\n",
    "        # Interpreted Implementation\n",
    "\n",
    "        if learning_mode == \"q_learning\":\n",
    "            for idx, trajectory in enumerate(trajectories):\n",
    "                # partner selection training\n",
    "                q_learning(self.qtable_ps, self.qtable_pd, trajectory[0], trajectory[1], trajectory[2], trajectory[3], self.a, self.g)\n",
    "                # prisoner's dilemma training\n",
    "                q_learning(self.qtable_pd, self.qtable_ps, trajectory[3], trajectory[4], trajectory[6], trajectory[5], self.a, self.g)\n",
    "        elif learning_mode == \"sarsa\":\n",
    "            for idx, trajectory in enumerate(trajectories):\n",
    "                # partner selection training\n",
    "                sarsa_learning(self.qtable_ps, self.qtable_pd, trajectory[0], trajectory[1], trajectory[2], trajectory[3], trajectory[4], self.a, self.g)\n",
    "                if last_trajectory is not None:\n",
    "                    # prisoner's dilemma training\n",
    "                    sarsa_learning(self.qtable_pd, self.qtable_ps, last_trajectory[3], last_trajectory[4], last_trajectory[6], last_trajectory[5], trajectory[1], self.a, self.g)\n",
    "                last_trajectory = trajectory\n",
    "                \n",
    "        # decrease temperature\n",
    "        # self.t *= 0.01\n",
    "        self.t *= self.delta_t\n",
    "    \n",
    "    def get_strategy_ps(self):\n",
    "        if (self.qtable_ps[0, 0] < self.qtable_ps[0, 1] and self.qtable_ps[1, 0] < self.qtable_ps[1, 1]):\n",
    "            return StrategyPS.ALWAYS_STAY\n",
    "        elif (self.qtable_ps[0, 0] > self.qtable_ps[0, 1] and self.qtable_ps[1, 0] < self.qtable_ps[1, 1]):\n",
    "            return StrategyPS.OUT_FOR_TAT\n",
    "        elif (self.qtable_ps[0, 0] < self.qtable_ps[0, 1] and self.qtable_ps[1, 0] > self.qtable_ps[1, 1]):\n",
    "            return StrategyPS.REVERSE_OUT_FOR_TAT\n",
    "        elif (self.qtable_ps[0, 0] > self.qtable_ps[0, 1] and self.qtable_ps[1, 0] > self.qtable_ps[1, 1]):\n",
    "            return StrategyPS.ALWAYS_LEAVE\n",
    "        else:\n",
    "            return StrategyPS.RANDOM\n",
    "\n",
    "    def get_strategy_pd(self):\n",
    "        if (self.qtable_pd[0, 0] < self.qtable_pd[0, 1] and self.qtable_pd[1, 0] < self.qtable_pd[1, 1]):\n",
    "            return StrategyPD.ALWAYS_COOPERATE\n",
    "        elif (self.qtable_pd[0, 0] > self.qtable_pd[0, 1] and self.qtable_pd[1, 0] < self.qtable_pd[1, 1]):\n",
    "            return StrategyPD.TIT_FOR_TAT\n",
    "        elif (self.qtable_pd[0, 0] < self.qtable_pd[0, 1] and self.qtable_pd[1, 0] > self.qtable_pd[1, 1]):\n",
    "            return StrategyPD.REVERSE_TIT_FOR_TAT\n",
    "        elif (self.qtable_pd[0, 0] > self.qtable_pd[0, 1] and self.qtable_pd[1, 0] > self.qtable_pd[1, 1]):\n",
    "            return StrategyPD.ALWAYS_DEFECT\n",
    "        else:\n",
    "            return StrategyPD.RANDOM\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the rewards of two agents in the prisoner's dilemma game\n",
    "def prisoners_dilemma(a_i: ActionPD, a_j: ActionPD) -> tuple[float, float]:\n",
    "    reward_table = np.array([[(1, 1), (5, 0)], [(0, 5), (3, 3)]])\n",
    "    return reward_table[a_i.value, a_j.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectories\n",
    "Each agent tracks the folowing figures each round as a trajectory:  \n",
    "- $(s_{PS}, a_{PS}, r_{PS} = 0, s'_{PS} = s_{PD}, a_{PD}, s'_{PD}, r_{PD})$\n",
    "- $s_{PS}$ is the state of the agent in the partner selection stage.  \n",
    "- $a_{PS}$ is the action the agent made in the partner selection stage.  \n",
    "- $r_{PS}$ is the reward the agent recieved from the partner selection stage. ~~This will always be zero and is not recorded~~.  \n",
    "- $s'_{PS}$ is the new state after the prisoner selection stage which is equivalent to the state in the prisoner's dilemma stage $s_{PD}$.  \n",
    "- $a_{PD}$ is the action the agent made in the prisoner's dilemma stage.  \n",
    "- $s'_{PS}$ is the new state after the prisoner's dilemma stage.  \n",
    "- $r_{PD}$ is the reward the agent recieved from the prisoners dilemma stage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdoo(population: int, rounds: int, episodes: int, learning_rate: float, temperature: float, discount_rate: float, delta_t: float,\n",
    "         disposition: float, know_fresh_agent: float, prefer_same_pool: float, prefer_different_pool: float, learning_mode: str = \"q_learning\",\n",
    "         do_plot: bool = True):\n",
    "    if (population % 2 != 0):\n",
    "        print(\"sdoo: population must be a multiple of two\")\n",
    "        return\n",
    "    if prefer_same_pool + prefer_different_pool > 1.0:\n",
    "        print(\"sdoo: prefer_same_pool + prefer_different_pool must be less than or equal to 1.0\")\n",
    "        return\n",
    "    \n",
    "    recorded_outcomes_pd = {\n",
    "        (ActionPD.DEFECT, ActionPD.DEFECT): [0 for _ in range(episodes)],\n",
    "        (ActionPD.DEFECT, ActionPD.COOPERATE): [0 for _ in range(episodes)],\n",
    "        (ActionPD.COOPERATE, ActionPD.DEFECT): [0 for _ in range(episodes)],\n",
    "        (ActionPD.COOPERATE, ActionPD.COOPERATE): [0 for _ in range(episodes)],\n",
    "    }\n",
    "\n",
    "    recorded_agent_strategy_pairings = {\n",
    "        # (a, b): [0 for _ in range(episodes)] for a in StrategyPD for b in StrategyPD\n",
    "        (StrategyPD.ALWAYS_COOPERATE, StrategyPD.ALWAYS_COOPERATE): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.ALWAYS_COOPERATE, StrategyPD.TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.ALWAYS_COOPERATE, StrategyPD.REVERSE_TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.ALWAYS_COOPERATE, StrategyPD.ALWAYS_DEFECT): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.ALWAYS_COOPERATE, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.TIT_FOR_TAT, StrategyPD.TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.TIT_FOR_TAT, StrategyPD.REVERSE_TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.TIT_FOR_TAT, StrategyPD.ALWAYS_DEFECT): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.TIT_FOR_TAT, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.REVERSE_TIT_FOR_TAT, StrategyPD.REVERSE_TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.REVERSE_TIT_FOR_TAT, StrategyPD.ALWAYS_DEFECT): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.REVERSE_TIT_FOR_TAT, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.ALWAYS_DEFECT, StrategyPD.ALWAYS_DEFECT): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.ALWAYS_DEFECT, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.RANDOM, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "    }\n",
    "\n",
    "    recorded_outcome_changes = {\n",
    "        ((ActionPD.COOPERATE, ActionPD.COOPERATE), (ActionPD.COOPERATE, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.COOPERATE), (ActionPD.COOPERATE, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.COOPERATE), (ActionPD.DEFECT, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.COOPERATE), (ActionPD.DEFECT, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "\n",
    "        ((ActionPD.COOPERATE, ActionPD.DEFECT), (ActionPD.COOPERATE, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.DEFECT), (ActionPD.COOPERATE, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.DEFECT), (ActionPD.DEFECT, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.DEFECT), (ActionPD.DEFECT, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "\n",
    "        ((ActionPD.DEFECT, ActionPD.COOPERATE), (ActionPD.COOPERATE, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.COOPERATE), (ActionPD.COOPERATE, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.COOPERATE), (ActionPD.DEFECT, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.COOPERATE), (ActionPD.DEFECT, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "\n",
    "        ((ActionPD.DEFECT, ActionPD.DEFECT), (ActionPD.COOPERATE, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.DEFECT), (ActionPD.COOPERATE, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.DEFECT), (ActionPD.DEFECT, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.DEFECT), (ActionPD.DEFECT, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "    }\n",
    "\n",
    "    agent_qvales_ps = [[0 for _ in range(episodes)] for _ in range(4)]\n",
    "    agent_qvales_pd = [[0 for _ in range(episodes)] for _ in range(4)]\n",
    "\n",
    "    recorded_qvalues_ps = [[[0 for _ in range(episodes)] for _ in range(4)] for _ in range(population)]\n",
    "    recorded_qvalues_pd = [[[0 for _ in range(episodes)] for _ in range(4)] for _ in range(population)]\n",
    "\n",
    "    agent_ps_actions_per_episode = [[0 for _ in range(episodes)] for _ in range(4)]\n",
    "    percentage_of_states_per_episode = [[0 for _ in range(episodes)] for _ in range(4)]\n",
    "\n",
    "    agent_chosen_switches_per_episode = [0 for _ in range(episodes)]\n",
    "    agent_switches_per_episode = [0 for _ in range(episodes)]\n",
    "\n",
    "    total_reward = [0 for _ in range(episodes)]\n",
    "\n",
    "    # Fix Randoms\n",
    "    # np.random.seed(0)\n",
    "\n",
    "    # Global Q-Table Test\n",
    "    # qtable_ps = np.zeros((2, 2))\n",
    "    # qtable_pd = np.zeros((2, 2))\n",
    "    # agents = [Agent(learning_rate, temperature, discount_rate, qtable_ps=qtable_ps, qtable_pd=qtable_pd) for _ in range(population)]\n",
    "\n",
    "    agents = [Agent(learning_rate, temperature, discount_rate, delta_t=delta_t) for _ in range(population)]\n",
    "    unpaired = list(range(population))\n",
    "\n",
    "# Pair Agents\n",
    "    pairs: tuple[int, int] = []\n",
    "    while unpaired:\n",
    "        i = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "        j = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "        pairs.append((i, j))\n",
    "\n",
    "    probabilities_ps_defected = []\n",
    "    probabilities_ps_cooperated = []\n",
    "    probabilities_pd_defected = []\n",
    "    probabilities_pd_cooperated = []\n",
    "    new_probabilities_ps_defected = []\n",
    "    new_probabilities_pd_defected = []\n",
    "    new_probabilities_ps_cooperated = []\n",
    "    new_probabilities_pd_cooperated = []\n",
    "    strategies_ps = []\n",
    "    strategies_pd = []\n",
    "    new_strategies_ps = []\n",
    "    new_strategies_pd = []\n",
    "    \n",
    "    # probabilities_ps_i.append()\n",
    "    # probabilities_ps_j.append(0.1)\n",
    "    # probabilities_pd_i.append(0.1)\n",
    "    # probabilities_pd_j.append(0.1)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Record agent Q-Values\n",
    "        for agent_idx in range(len(recorded_qvalues_ps)):\n",
    "            for idx in range(len(recorded_qvalues_ps[agent_idx])):\n",
    "                recorded_qvalues_ps[agent_idx][idx][episode] = agents[agent_idx].qtable_ps.ravel()[idx]\n",
    "\n",
    "        for agent_idx in range(len(recorded_qvalues_pd)):\n",
    "            for idx in range(len(recorded_qvalues_pd[agent_idx])):\n",
    "                recorded_qvalues_pd[agent_idx][idx][episode] = agents[agent_idx].qtable_pd.ravel()[idx]\n",
    "\n",
    "        trajectories = [[] for _ in range(population)]\n",
    "        for round in range(rounds):\n",
    "\n",
    "            # Partner Selection\n",
    "            temp_pairs = []\n",
    "            switch_leave_pool = []\n",
    "            switch_stay_pool = []\n",
    "            stay_pool = []\n",
    "\n",
    "            for (i, j) in pairs:\n",
    "                s_i = get_state(agents[j].last_action_pd)\n",
    "                s_j = get_state(agents[i].last_action_pd)\n",
    "                percentage_of_states_per_episode[s_i.value][episode] += 1\n",
    "                percentage_of_states_per_episode[s_j.value][episode] += 1\n",
    "\n",
    "                a_i = agents[i].get_action_ps(s_i)\n",
    "                a_j = agents[j].get_action_ps(s_j)\n",
    "\n",
    "                tempi_ps = boltzmann_exploration(agents[i].qtable_ps, s_i, agents[i].t, 0.8)\n",
    "                tempj_ps = boltzmann_exploration(agents[j].qtable_ps, s_j, agents[j].t, 0.8)\n",
    "\n",
    "                if s_i == State.PARTNER_DEFECTED:\n",
    "                    probabilities_ps_defected.append(tempi_ps[0])\n",
    "                else:\n",
    "                    probabilities_ps_cooperated.append(tempi_ps[0])\n",
    "                if s_j == State.PARTNER_DEFECTED:\n",
    "                    probabilities_ps_defected.append(tempj_ps[0])\n",
    "                else:\n",
    "                    probabilities_ps_cooperated.append(tempj_ps[0])\n",
    "\n",
    "                if a_i == ActionPS.LEAVE or a_j == ActionPS.LEAVE:\n",
    "                    if a_i == ActionPS.LEAVE:\n",
    "                        switch_leave_pool.append(i)\n",
    "                        agents[i].last_action_ps = ActionPS.LEAVE\n",
    "                        agents[i].last_result_ps = \"split\"\n",
    "                    else:\n",
    "                        switch_stay_pool.append(i)\n",
    "                        agents[i].last_action_ps = ActionPS.STAY\n",
    "                        agents[i].last_result_ps = \"split\"\n",
    "                    if a_j == ActionPS.LEAVE:\n",
    "                        switch_leave_pool.append(j)\n",
    "                        agents[j].last_action_ps = ActionPS.LEAVE\n",
    "                        agents[j].last_result_ps = \"split\"\n",
    "                    else:\n",
    "                        switch_stay_pool.append(j)\n",
    "                        agents[j].last_action_ps = ActionPS.STAY\n",
    "                        agents[j].last_result_ps = \"split\"\n",
    "                    agent_switches_per_episode[episode] += 2\n",
    "                    agent_chosen_switches_per_episode[episode] += int(a_i == ActionPS.LEAVE) + int(a_j == ActionPS.LEAVE)\n",
    "                else:\n",
    "                    stay_pool.append((i, j))\n",
    "                    agents[i].last_action_ps = ActionPS.STAY\n",
    "                    agents[i].last_result_ps = \"stay\"\n",
    "                    agents[j].last_action_ps = ActionPS.STAY\n",
    "                    agents[j].last_result_ps = \"stay\"\n",
    "\n",
    "                # Partner Selection Rewards Test\n",
    "                # r_i = 1 if a_j == ActionPS.STAY else -1\n",
    "                # r_j = 1 if a_i == ActionPS.STAY else -1\n",
    "                r_i = 0\n",
    "                r_j = 0\n",
    "\n",
    "                trajectories[i].append((s_i, a_i, r_i))\n",
    "                trajectories[j].append((s_j, a_j, r_j))\n",
    "\n",
    "            # Pair Agents in switch_pool and switched_pool\n",
    "            while switch_leave_pool and switch_stay_pool:\n",
    "                full_pool = switch_leave_pool + switch_stay_pool\n",
    "                i = full_pool.pop(np.random.randint(len(full_pool)))\n",
    "                i_pool = switch_leave_pool if i in switch_leave_pool else switch_stay_pool\n",
    "                i_pool.remove(i)\n",
    "                other_pool = switch_leave_pool if i_pool == switch_stay_pool else switch_stay_pool\n",
    "                rand_choice = np.random.rand()\n",
    "                if rand_choice < prefer_same_pool and i_pool:\n",
    "                    j_pool = i_pool\n",
    "                    j = j_pool.pop(np.random.randint(len(j_pool)))\n",
    "                elif rand_choice < prefer_same_pool + prefer_different_pool and other_pool:\n",
    "                    j_pool = other_pool\n",
    "                    j = j_pool.pop(np.random.randint(len(j_pool)))\n",
    "                else:\n",
    "                    j = full_pool.pop(np.random.randint(len(full_pool)))\n",
    "                    j_pool = switch_leave_pool if j in switch_leave_pool else switch_stay_pool\n",
    "                    j_pool.remove(j)\n",
    "                temp_pairs.append((i, j))\n",
    "\n",
    "            # Pair remaining agents in switch_pool with each other\n",
    "            while len(switch_leave_pool) > 1:\n",
    "                i = switch_leave_pool.pop(np.random.randint(len(switch_leave_pool)))\n",
    "                j = switch_leave_pool.pop(np.random.randint(len(switch_leave_pool)))\n",
    "                temp_pairs.append((i, j))\n",
    "\n",
    "            # Pair remaining agents in switched_pool with each other\n",
    "            while len(switch_stay_pool) > 1:\n",
    "                i = switch_stay_pool.pop(np.random.randint(len(switch_stay_pool)))\n",
    "                j = switch_stay_pool.pop(np.random.randint(len(switch_stay_pool)))\n",
    "                temp_pairs.append((i, j))\n",
    "\n",
    "            # Combine new pairs with stay_pool pairs\n",
    "            pairs = temp_pairs + stay_pool\n",
    "\n",
    "            # Prisoner's Dilemma\n",
    "            for (i, j) in pairs:\n",
    "                strategy_i = agents[i].get_strategy_pd()\n",
    "                strategy_j = agents[j].get_strategy_pd()\n",
    "                if (strategy_i, strategy_j) in recorded_agent_strategy_pairings:\n",
    "                    recorded_agent_strategy_pairings[(strategy_i, strategy_j)][episode] += 1\n",
    "                elif (strategy_j, strategy_i) in recorded_agent_strategy_pairings:\n",
    "                    recorded_agent_strategy_pairings[(strategy_j, strategy_i)][episode] += 1\n",
    "\n",
    "                # If agent split, use random action\n",
    "                if (agents[i].last_result_ps == \"split\" and agents[j].last_result_ps == \"split\") and np.random.rand() > know_fresh_agent:\n",
    "                    s_i = State.PARTNER_COOPERATED if np.random.rand() < disposition else State.PARTNER_DEFECTED\n",
    "                    s_j = State.PARTNER_COOPERATED if np.random.rand() < disposition else State.PARTNER_DEFECTED\n",
    "                else:\n",
    "                    s_i = get_state(agents[j].last_action_pd)\n",
    "                    s_j = get_state(agents[i].last_action_pd)\n",
    "                percentage_of_states_per_episode[(s_i.value) + 2][episode] += 1\n",
    "                percentage_of_states_per_episode[(s_j.value) + 2][episode] += 1\n",
    "\n",
    "                a_i = agents[i].get_action_pd(s_i)\n",
    "                a_j = agents[j].get_action_pd(s_j)\n",
    "\n",
    "                tempi_pd = boltzmann_exploration(agents[i].qtable_pd, s_i, agents[i].t, 0.8)\n",
    "                tempj_pd = boltzmann_exploration(agents[j].qtable_pd, s_j, agents[j].t, 0.8)\n",
    "\n",
    "                if s_i == State.PARTNER_DEFECTED:\n",
    "                    probabilities_pd_defected.append(tempi_pd[0])\n",
    "                else:\n",
    "                    probabilities_pd_cooperated.append(tempi_pd[0])\n",
    "                if s_j == State.PARTNER_DEFECTED:\n",
    "                    probabilities_pd_defected.append(tempj_pd[0])\n",
    "                else:\n",
    "                    probabilities_pd_cooperated.append(tempj_pd[0])\n",
    "\n",
    "                r_i, r_j = prisoners_dilemma(a_i, a_j)\n",
    "                total_reward[episode] += r_i + r_j\n",
    "\n",
    "                ns_i = get_state(a_j)\n",
    "                ns_j = get_state(a_i)\n",
    "                recorded_outcomes_pd[(a_i, a_j)][episode] += 1\n",
    "                agents[i].last_action_pd = a_i\n",
    "                agents[j].last_action_pd = a_j\n",
    "\n",
    "                # Record Trajectories\n",
    "                t = trajectories[i][round]\n",
    "                trajectories[i][round] = (t[0], t[1], t[2], s_i, a_i, ns_i, r_i)\n",
    "                t = trajectories[j][round]\n",
    "                trajectories[j][round] = (t[0], t[1], t[2], s_j, a_j, ns_j, r_j)\n",
    "\n",
    "                # Record Actions taken\n",
    "                agent_ps_actions_per_episode[2 * (s_i.value - 2) + a_i.value][episode] += 1\n",
    "                agent_ps_actions_per_episode[2 * (s_j.value - 2) + a_j.value][episode] += 1\n",
    "\n",
    "        # print()\n",
    "        \n",
    "        for idx, agent in enumerate(agents):\n",
    "            agent.train(trajectories[idx], learning_mode, last_trajectory=agent.last_trajectory)\n",
    "            # agent.train(trajectories[idx], debug=(idx == 0))\n",
    "        for idx, agent in enumerate(agents):\n",
    "            agent.last_trajectory = trajectories[idx][-1]\n",
    "        \n",
    "        for idx in range(len(agent_qvales_ps)):\n",
    "            agent_qvales_ps[idx][episode] = np.sum([agent.qtable_ps.ravel()[idx] for agent in agents]) / len(agents)\n",
    "\n",
    "        for idx in range(len(agent_qvales_pd)):\n",
    "            agent_qvales_pd[idx][episode] = np.sum([agent.qtable_pd.ravel()[idx] for agent in agents]) / len(agents)\n",
    "\n",
    "        recorded_outcomes_pd[(ActionPD.DEFECT, ActionPD.DEFECT)][episode] /= (rounds * population / 2)\n",
    "        recorded_outcomes_pd[(ActionPD.DEFECT, ActionPD.COOPERATE)][episode] /= (rounds * population / 2)\n",
    "        recorded_outcomes_pd[(ActionPD.COOPERATE, ActionPD.DEFECT)][episode] /= (rounds * population / 2)\n",
    "        recorded_outcomes_pd[(ActionPD.COOPERATE, ActionPD.COOPERATE)][episode] /= (rounds * population / 2)\n",
    "\n",
    "        agent_ps_actions_per_episode[0][episode] /= (rounds * population)\n",
    "        agent_ps_actions_per_episode[1][episode] /= (rounds * population)\n",
    "        agent_ps_actions_per_episode[2][episode] /= (rounds * population)\n",
    "        agent_ps_actions_per_episode[3][episode] /= (rounds * population)\n",
    "\n",
    "        percentage_of_states_per_episode[0][episode] /= (rounds * population)\n",
    "        percentage_of_states_per_episode[1][episode] /= (rounds * population)\n",
    "        percentage_of_states_per_episode[2][episode] /= (rounds * population)\n",
    "        percentage_of_states_per_episode[3][episode] /= (rounds * population)\n",
    "\n",
    "        for idx, agent in enumerate(agents):\n",
    "            strategies_ps.append(agent.get_strategy_ps())\n",
    "            strategies_pd.append(agent.get_strategy_pd())\n",
    "\n",
    "        for agent_trajectories in trajectories:\n",
    "            for idx in range(rounds - 1):\n",
    "                outcome = (agent_trajectories[idx][4], ActionPD.COOPERATE if agent_trajectories[idx][5] == State.PARTNER_COOPERATED else ActionPD.DEFECT)\n",
    "                next_outcome = (agent_trajectories[idx + 1][4], ActionPD.COOPERATE if agent_trajectories[idx + 1][5] == State.PARTNER_COOPERATED else ActionPD.DEFECT)\n",
    "                # if (outcome, next_outcome) in recorded_outcome_changes:\n",
    "                recorded_outcome_changes[(outcome, next_outcome)][episode] += 1\n",
    "                # else:\n",
    "                #     recorded_outcome_changes[((outcome[1], outcome[0]), (next_outcome[1], next_outcome[0]))][episode] += 1\n",
    "        mean_probabilities_ps_defected = np.mean(probabilities_ps_defected)\n",
    "        mean_probabilities_ps_cooperated = np.mean(probabilities_ps_cooperated)\n",
    "        mean_probabilities_pd_defected = np.mean(probabilities_pd_defected)\n",
    "        mean_probabilities_pd_cooperated = np.mean(probabilities_pd_cooperated)\n",
    "        new_probabilities_ps_defected.append(mean_probabilities_ps_defected)\n",
    "        new_probabilities_ps_cooperated.append(mean_probabilities_ps_cooperated)\n",
    "        new_probabilities_pd_defected.append(mean_probabilities_pd_defected)\n",
    "        new_probabilities_pd_cooperated.append(mean_probabilities_pd_cooperated)\n",
    "        new_strategies_ps.append(strategies_ps)\n",
    "        new_strategies_pd.append(strategies_pd)\n",
    "        probabilities_ps_defected = []\n",
    "        probabilities_ps_cooperated = []\n",
    "        probabilities_pd_defected = []\n",
    "        probabilities_pd_cooperated = []\n",
    "        strategies_ps = []\n",
    "        strategies_pd = []\n",
    "\n",
    "    strat_always_leave = [sum([1 for strategy in strategies if strategy == StrategyPS.ALWAYS_LEAVE]) for strategies in new_strategies_ps]\n",
    "    strat_out_for_tat = [sum([1 for strategy in strategies if strategy == StrategyPS.OUT_FOR_TAT]) for strategies in new_strategies_ps]\n",
    "    strat_reverse_out_for_tat = [sum([1 for strategy in strategies if strategy == StrategyPS.REVERSE_OUT_FOR_TAT]) for strategies in new_strategies_ps]\n",
    "    strat_always_stay = [sum([1 for strategy in strategies if strategy == StrategyPS.ALWAYS_STAY]) for strategies in new_strategies_ps]\n",
    "    strat_always_defect = [sum([1 for strategy in strategies if strategy == StrategyPD.ALWAYS_DEFECT]) for strategies in new_strategies_pd]\n",
    "    strat_tit_for_tat = [sum([1 for strategy in strategies if strategy == StrategyPD.TIT_FOR_TAT]) for strategies in new_strategies_pd]\n",
    "    strat_reverse_tit_for_tat = [sum([1 for strategy in strategies if strategy == StrategyPD.REVERSE_TIT_FOR_TAT]) for strategies in new_strategies_pd]\n",
    "    strat_always_cooperate = [sum([1 for strategy in strategies if strategy == StrategyPD.ALWAYS_COOPERATE]) for strategies in new_strategies_pd]\n",
    "\n",
    "    # for agent_idx in range(recorded_qvalues_ps):\n",
    "    #         for idx in range(recorded_qvalues_ps[agent_idx]):\n",
    "    #             recorded_qvalues_ps[agent_idx][idx][episodes] = agent.qtable_ps.ravel()[idx]\n",
    "\n",
    "    num_strategies_ps = [0 for _ in StrategyPS]\n",
    "    num_strategies_pd = [0 for _ in StrategyPD]\n",
    "\n",
    "    strategy_combinations = np.zeros((len(StrategyPS), len(StrategyPD)))\n",
    "\n",
    "    # Determine Agent Strategies\n",
    "    for idx, agent in enumerate(agents):\n",
    "        strategy_ps = agent.get_strategy_ps()\n",
    "        strategy_pd = agent.get_strategy_pd()\n",
    "        \n",
    "        num_strategies_ps[strategy_ps.value] += 1\n",
    "        num_strategies_pd[strategy_pd.value] += 1\n",
    "        strategy_combinations[strategy_ps.value, strategy_pd.value] += 1\n",
    "        \n",
    "        print(\"Agent %i) PS-Strategy: %s, PD-Strategy: %s\" % \n",
    "            (idx, strategy_names[strategy_ps], strategy_names[strategy_pd]))\n",
    "        print(agent.qtable_ps)\n",
    "        print(agent.qtable_pd)\n",
    "\n",
    "    results = {\n",
    "        \"agents\": agents,\n",
    "        \"recorded_outcomes_pd\": recorded_outcomes_pd,\n",
    "        \"recorded_agent_strategy_pairings\": recorded_agent_strategy_pairings,\n",
    "        \"recorded_outcome_changes\": recorded_outcome_changes,\n",
    "        \"agent_qvales_ps\": agent_qvales_ps,\n",
    "        \"agent_qvales_pd\": agent_qvales_pd,\n",
    "        \"recorded_qvalues_ps\": recorded_qvalues_ps,\n",
    "        \"recorded_qvalues_pd\": recorded_qvalues_pd,\n",
    "        \"agent_ps_actions_per_episode\": agent_ps_actions_per_episode,\n",
    "        \"percentage_of_states_per_episode\": percentage_of_states_per_episode,\n",
    "        \"agent_chosen_switches_per_episode\": agent_chosen_switches_per_episode,\n",
    "        \"agent_switches_per_episode\": agent_switches_per_episode,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"ps_strategies\": {\n",
    "            StrategyPS.ALWAYS_LEAVE: strat_always_leave,\n",
    "            StrategyPS.OUT_FOR_TAT: strat_out_for_tat,\n",
    "            StrategyPS.REVERSE_OUT_FOR_TAT: strat_reverse_out_for_tat,\n",
    "            StrategyPS.ALWAYS_STAY: strat_always_stay,\n",
    "        },\n",
    "        \"pd_strategies\": {\n",
    "            StrategyPD.ALWAYS_DEFECT: strat_always_defect,\n",
    "            StrategyPD.TIT_FOR_TAT: strat_tit_for_tat,\n",
    "            StrategyPD.REVERSE_TIT_FOR_TAT: strat_reverse_tit_for_tat,\n",
    "            StrategyPD.ALWAYS_COOPERATE: strat_always_cooperate,\n",
    "        },\n",
    "        \"new_probabilities_ps_defected\": new_probabilities_ps_defected,\n",
    "        \"new_probabilities_ps_cooperated\": new_probabilities_ps_cooperated,\n",
    "        \"new_probabilities_pd_defected\": new_probabilities_pd_defected,\n",
    "        \"new_probabilities_pd_cooperated\": new_probabilities_pd_cooperated,\n",
    "        \"population\": population,\n",
    "        \"rounds\": rounds,\n",
    "        \"num_strategies_ps\": num_strategies_ps,\n",
    "        \"num_strategies_pd\": num_strategies_pd,\n",
    "        \"strategy_combinations\": strategy_combinations,\n",
    "    }\n",
    "\n",
    "    if do_plot:\n",
    "        plot_all(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_all(results):\n",
    "    plot_pd_outcomes(results)\n",
    "    plot_strategies(results)\n",
    "    plot_mean_probabilities(results)\n",
    "    plot_rewards(results)\n",
    "    plot_agent_switches_per_episode(results)\n",
    "    plot_percentage_of_pd_actions_per_episode(results)\n",
    "    plot_percentage_of_states_per_episode(results)\n",
    "    plot_final_strategies(results)\n",
    "    plot_strategy_combinations(results)\n",
    "    plot_agent_strategy_pairings(results)\n",
    "    plot_average_qvalues(results)\n",
    "    plot_agents_qvalues(results)\n",
    "    plot_outcome_changes(results)\n",
    "\n",
    "def plot_pd_outcomes(results):\n",
    "    recorded_outcomes_pd = results[\"recorded_outcomes_pd\"]\n",
    "    # Plot Prisoner's Dilemma Outcomes\n",
    "    plt.plot(recorded_outcomes_pd[(ActionPD.DEFECT, ActionPD.DEFECT)], linewidth=1)\n",
    "    plt.plot(recorded_outcomes_pd[(ActionPD.DEFECT, ActionPD.COOPERATE)], linewidth=1)\n",
    "    plt.plot(recorded_outcomes_pd[(ActionPD.COOPERATE, ActionPD.DEFECT)], linewidth=1)\n",
    "    plt.plot(recorded_outcomes_pd[(ActionPD.COOPERATE, ActionPD.COOPERATE)], linewidth=1)\n",
    "    \n",
    "    plt.title(\"Percentage of Prisoner's Dilemma Outcomes Per Episode\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Percentage of Outcomes')\n",
    "    plt.legend([\"(D, D)\", \"(D, C)\", \"(C, D)\", \"(C, C)\", \"Total Reward\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_strategies(results, do_ps=True, do_pd=True):\n",
    "    strat_always_leave = results[\"ps_strategies\"][StrategyPS.ALWAYS_LEAVE]\n",
    "    strat_out_for_tat = results[\"ps_strategies\"][StrategyPS.OUT_FOR_TAT]\n",
    "    strat_reverse_out_for_tat = results[\"ps_strategies\"][StrategyPS.REVERSE_OUT_FOR_TAT]\n",
    "    strat_always_stay = results[\"ps_strategies\"][StrategyPS.ALWAYS_STAY]\n",
    "    strat_always_defect = results[\"pd_strategies\"][StrategyPD.ALWAYS_DEFECT]\n",
    "    strat_tit_for_tat = results[\"pd_strategies\"][StrategyPD.TIT_FOR_TAT]\n",
    "    strat_reverse_tit_for_tat = results[\"pd_strategies\"][StrategyPD.REVERSE_TIT_FOR_TAT]\n",
    "    strat_always_cooperate = results[\"pd_strategies\"][StrategyPD.ALWAYS_COOPERATE]\n",
    "\n",
    "    if do_ps:\n",
    "        plt.plot(strat_always_leave, linewidth=1)\n",
    "        plt.plot(strat_out_for_tat, linewidth=1)\n",
    "        plt.plot(strat_reverse_out_for_tat, linewidth=1)\n",
    "        plt.plot(strat_always_stay, linewidth=1)\n",
    "\n",
    "        plt.title(\"Number of Partner Selection Strategies Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Number of Agents')\n",
    "        plt.legend([\"Always Leave\", \"Out For Tat\", \"Reverse Out For Tat\", \"Always Stay\"])\n",
    "        plt.show()\n",
    "\n",
    "    if do_pd:\n",
    "        plt.plot(strat_always_defect)\n",
    "        plt.plot(strat_tit_for_tat)\n",
    "        plt.plot(strat_reverse_tit_for_tat)\n",
    "        plt.plot(strat_always_cooperate)\n",
    "\n",
    "        plt.title(\"Number of Prisoner's Dilemma Strategies Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Number of Agents')\n",
    "        plt.legend([\"Always Defect\", \"Tit For Tat\", \"Reverse Tit For Tat\", \"Always Cooperate\"])\n",
    "        plt.show()\n",
    "\n",
    "def plot_mean_probabilities(results, do_ps=True, do_pd=True):\n",
    "    new_probabilities_pd_defected = results[\"new_probabilities_pd_defected\"]\n",
    "    new_probabilities_pd_cooperated = results[\"new_probabilities_pd_cooperated\"]\n",
    "    new_probabilities_ps_defected = results[\"new_probabilities_ps_defected\"]\n",
    "    new_probabilities_ps_cooperated = results[\"new_probabilities_ps_cooperated\"]\n",
    "\n",
    "    if do_pd:\n",
    "        # Plot Mean Probabilities of Defection and Cooperation\n",
    "        plt.title(\"Probabilities of defection given state in Prisoner's Dilemma Stage\")\n",
    "        plt.plot(new_probabilities_pd_defected)\n",
    "        plt.plot(new_probabilities_pd_cooperated)\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.legend([\"Partner Defected\", \"Partner Cooperated\"])\n",
    "        plt.ylabel(\"Probability of Defection\")\n",
    "        plt.show()\n",
    "\n",
    "    if do_ps:\n",
    "        plt.title(\"Probabilities of leaving given state in Partner Selection Stage\")\n",
    "        plt.plot(new_probabilities_ps_defected)\n",
    "        plt.plot(new_probabilities_ps_cooperated)\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.legend([\"Partner Defected\", \"Partner Cooperated\"])\n",
    "        plt.ylabel(\"Probability of Leaving\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "def plot_rewards(results):\n",
    "    total_reward = results[\"total_reward\"]\n",
    "    population = results[\"population\"]\n",
    "    rounds = results[\"rounds\"]\n",
    "    # Plot Total Rewards\n",
    "    plt.plot(total_reward, linewidth=3)\n",
    "\n",
    "    plt.title(\"Total Reward Per Episode\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.ylim(0, 6 * population * rounds / 2)\n",
    "    plt.show()\n",
    "\n",
    "def plot_agent_switches_per_episode(results):\n",
    "    agent_chosen_switches_per_episode = results[\"agent_chosen_switches_per_episode\"]\n",
    "    agent_switches_per_episode = results[\"agent_switches_per_episode\"]\n",
    "    population = results[\"population\"]\n",
    "    rounds = results[\"rounds\"]\n",
    "    \n",
    "    # Plot Agent/Pair Switches Per Episode\n",
    "    plt.plot(np.divide(agent_chosen_switches_per_episode, rounds * population))\n",
    "    plt.plot(np.divide(agent_switches_per_episode, rounds * population))\n",
    "\n",
    "    plt.title(\"Percentage of Switches Per Episode\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Percentage of Agents')\n",
    "    plt.legend([\"Agents Who Chose to Switch Partners\", \"Agents Who Switched Partners\"])\n",
    "    plt.show()\n",
    "\n",
    "def plot_percentage_of_pd_actions_per_episode(results):\n",
    "    agent_pd_actions_per_episode = results[\"agent_pd_actions_per_episode\"]\n",
    "    # Plot Percentage of Agent PD Actions Per Episode Given State\n",
    "    plt.plot(agent_pd_actions_per_episode[0])\n",
    "    plt.plot(agent_pd_actions_per_episode[1])\n",
    "    plt.plot(agent_pd_actions_per_episode[2])\n",
    "    plt.plot(agent_pd_actions_per_episode[3])\n",
    "    \n",
    "    plt.title(\"Percentage of PD Actions Per Episode Given State\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Percentage of Agents')\n",
    "    plt.legend([\"Defected Given Parter Previously Defected\", \"Cooperated Given Parter Previously Defected\", \n",
    "                \"Defected Given Parter Previously Cooperated\", \"Cooperated Given Parter Previously Cooperated\"],\n",
    "                loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "def plot_percentage_of_states_per_episode(results, do_ps=True, do_pd=True):\n",
    "    percentage_of_states_per_episode = results[\"percentage_of_states_per_episode\"]\n",
    "    \n",
    "    if do_ps:\n",
    "        # Plot Percentage of Agent States Per Episode\n",
    "        plt.subplot(211)\n",
    "        plt.plot(percentage_of_states_per_episode[0])\n",
    "        plt.plot(percentage_of_states_per_episode[1])\n",
    "        \n",
    "        plt.title(\"Percentage of Partner Selection Agent States Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Percentage of States')\n",
    "        plt.legend([\"Partner Selection Where Parter Previously Defected\", \"Partner Selection Where Parter Previously Cooperated\"], \n",
    "                    loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "    if do_pd:\n",
    "        plt.subplot(212)\n",
    "        plt.plot(percentage_of_states_per_episode[2], linestyle='dotted')\n",
    "        plt.plot(percentage_of_states_per_episode[3], linestyle='dotted')\n",
    "        \n",
    "        plt.title(\"Percentage of Prisoner's Dilemma Agent States Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Percentage of States')\n",
    "        plt.legend([\"Prisoner's Dilemma Where Parter Previously Defected\", \"Prisoner's Dilemma Where Parter Previously Cooperated\"], \n",
    "                    loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "def plot_final_strategies(results, do_ps=True, do_pd=True):\n",
    "    num_strategies_ps = results[\"num_strategies_ps\"]\n",
    "    num_strategies_pd = results[\"num_strategies_pd\"]\n",
    "    population = results[\"population\"]\n",
    "\n",
    "    if do_ps:\n",
    "        plt.subplot(211)\n",
    "        ps_colors = [strategy_colors[strategy] for strategy in StrategyPS]\n",
    "        plt.bar([strategy_names[strategy] for strategy in StrategyPS], num_strategies_ps, color=ps_colors)\n",
    "        plt.title('PS-Strategies')\n",
    "        plt.xlabel('Strategy')\n",
    "        plt.ylabel('Number of Agents')\n",
    "        plt.ylim(0, population)\n",
    "        plt.show()\n",
    "\n",
    "    if do_pd:\n",
    "        plt.subplot(212)\n",
    "        pd_colors = [strategy_colors[strategy] for strategy in StrategyPD]\n",
    "        plt.bar([strategy_names[strategy] for strategy in StrategyPD], num_strategies_pd, color=pd_colors)\n",
    "        plt.title('PD-Strategies')\n",
    "        plt.xlabel('Strategy')\n",
    "        plt.ylabel('Number of Agents')\n",
    "        plt.ylim(0, population)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_strategy_combinations(results):\n",
    "    strategy_combinations = results[\"strategy_combinations\"]\n",
    "    population = results[\"population\"]\n",
    "    \n",
    "    # Plot agent partner selection and prisoner's dilemma strategy combinations\n",
    "    combination_indices = [i for i in range(len(strategy_combinations.ravel()))]\n",
    "    ps_combination_colors = np.repeat([strategy_colors[strategy] for strategy in StrategyPS], 5)\n",
    "    pd_combination_colors = np.tile([strategy_colors[strategy] for strategy in StrategyPD], 5)\n",
    "    plt.bar(combination_indices, strategy_combinations.ravel() / 2.0, color=ps_combination_colors)\n",
    "    plt.bar(combination_indices, strategy_combinations.ravel() / 2.0, bottom=(strategy_combinations.ravel() / 2.0), color=pd_combination_colors)\n",
    "    plt.title('Final Strategy Combinations')\n",
    "    plt.xlabel('Strategy Combination')\n",
    "    plt.ylabel('Number of Agents')\n",
    "    # plt.ylim(0, population)\n",
    "    plt.legend(\n",
    "        [plt.Rectangle((0, 0), 1, 1, color=value) for key, value in strategy_colors.items()],\n",
    "        [strategy_names[key] for key, value in strategy_colors.items()],\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 1.35),\n",
    "        ncol=5,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "def plot_agent_strategy_pairings(results):\n",
    "    recorded_agent_strategy_pairings = results[\"recorded_agent_strategy_pairings\"]\n",
    "    # Plot Prisoner's Dilemma Strategy Pairings\n",
    "    for pairing, values in recorded_agent_strategy_pairings.items():\n",
    "        plt.plot(values, '-', color=strategy_colors[pairing[0]], linewidth=3)\n",
    "        plt.plot(values, '--', color=strategy_colors[pairing[1]], linewidth=3)\n",
    "    \n",
    "    plt.title(\"Prisoner's Dilemma Strategy Pairings Per Episode\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Number of Pairings')\n",
    "    plt.legend(\n",
    "        [plt.Rectangle((0, 0), 1, 1, color=strategy_colors[strategy]) for strategy in StrategyPD],\n",
    "        [strategy_names[strategy] for strategy in StrategyPD],\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 1.35),\n",
    "        ncol=5,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_average_qvalues(results, do_ps=True, do_pd=True):\n",
    "    agent_qvales_ps = results[\"agent_qvales_ps\"]\n",
    "    agent_qvales_pd = results[\"agent_qvales_pd\"]\n",
    "    population = results[\"population\"]\n",
    "\n",
    "    if do_ps:\n",
    "        # Plot Average Partner Selection Q-Values in each episode\n",
    "        plt.plot(agent_qvales_ps[0])\n",
    "        plt.plot(agent_qvales_ps[1])\n",
    "        plt.plot(agent_qvales_ps[2], linestyle='dotted')\n",
    "        plt.plot(agent_qvales_ps[3], linestyle='dotted')\n",
    "        \n",
    "        plt.title(\"Partner Selection Q-Values in each episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Q-Value')\n",
    "        plt.legend([\"Leave Given Parter Previously Defected\", \"Stay Given Parter Previously Defected\", \n",
    "                    \"Leave Given Parter Previously Cooperated\", \"Stay Given Parter Previously Cooperated\"],\n",
    "                    loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "    if do_pd:\n",
    "        # Plot Average Prisoner's Dilemma Q-Values in each episode\n",
    "        plt.plot(agent_qvales_pd[0])\n",
    "        plt.plot(agent_qvales_pd[1])\n",
    "        plt.plot(agent_qvales_pd[2], linestyle='dotted')\n",
    "        plt.plot(agent_qvales_pd[3], linestyle='dotted')\n",
    "        \n",
    "        plt.title(\"Prisoner's Dilemma Q-Values in each episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Q-Value')\n",
    "        plt.legend([\"Defect Given Parter Previously Defected\", \"Cooperate Given Parter Previously Defected\", \n",
    "                    \"Defect Given Parter Previously Cooperated\", \"Cooperate Given Parter Previously Cooperated\"],\n",
    "                    loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "def plot_agents_qvalues(results, do_ps=False, do_pd=True):\n",
    "    recorded_qvalues_ps = results[\"recorded_qvalues_ps\"]\n",
    "    recorded_qvalues_pd = results[\"recorded_qvalues_pd\"]\n",
    "    population = results[\"population\"]\n",
    "    \n",
    "    # Plot Agents Partner Selection Q-Values in each episode\n",
    "    if do_ps:\n",
    "        for idx in range(population):\n",
    "            plt.subplot(211)\n",
    "            plt.plot(recorded_qvalues_ps[idx][0])\n",
    "            plt.plot(recorded_qvalues_ps[idx][1])\n",
    "            plt.plot(recorded_qvalues_ps[idx][2], linestyle='dotted')\n",
    "            plt.plot(recorded_qvalues_ps[idx][3], linestyle='dotted')\n",
    "            \n",
    "            plt.title(\"Agent \" + str(idx) + \" Partner Selection Q-Values in each episode\")\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Q-Value')\n",
    "            plt.legend([\"Leave Given Parter Previously Defected\", \"Stay Given Parter Previously Defected\", \n",
    "                        \"Leave Given Parter Previously Cooperated\", \"Stay Given Parter Previously Cooperated\"],\n",
    "                        loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "            plt.show()\n",
    "\n",
    "            plt.subplot(212)\n",
    "            plt.plot(recorded_qvalues_pd[idx][0])\n",
    "            plt.plot(recorded_qvalues_pd[idx][1])\n",
    "            plt.plot(recorded_qvalues_pd[idx][2], linestyle='dotted')\n",
    "            plt.plot(recorded_qvalues_pd[idx][3], linestyle='dotted')\n",
    "            \n",
    "            plt.title(\"Agent \" + str(idx) + \" Prisoner's Dilemma Q-Values in each episode\")\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Q-Value')\n",
    "            plt.legend([\"Defect Given Parter Previously Defected\", \"Cooperate Given Parter Previously Defected\", \n",
    "                        \"Defect Given Parter Previously Cooperated\", \"Cooperate Given Parter Previously Cooperated\"],\n",
    "                        loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "            plt.show()\n",
    "\n",
    "    # Plot Agents Prisoner's Dilemma Q-Values in each episode\n",
    "    if do_pd:\n",
    "        for idx in range(population):\n",
    "            plt.subplot(211)\n",
    "            plt.plot(recorded_qvalues_ps[idx][0])\n",
    "            plt.plot(recorded_qvalues_ps[idx][1])\n",
    "            plt.plot(recorded_qvalues_ps[idx][2], linestyle='dotted')\n",
    "            plt.plot(recorded_qvalues_ps[idx][3], linestyle='dotted')\n",
    "            \n",
    "            plt.title(\"Agent \" + str(idx) + \" Partner Selection Q-Values in each episode\")\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Q-Value')\n",
    "            plt.legend([\"Leave Given Parter Previously Defected\", \"Stay Given Parter Previously Defected\", \n",
    "                        \"Leave Given Parter Previously Cooperated\", \"Stay Given Parter Previously Cooperated\"],\n",
    "                        loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "            plt.show()\n",
    "\n",
    "            plt.subplot(212)\n",
    "            plt.plot(recorded_qvalues_pd[idx][0])\n",
    "            plt.plot(recorded_qvalues_pd[idx][1])\n",
    "            plt.plot(recorded_qvalues_pd[idx][2], linestyle='dotted')\n",
    "            plt.plot(recorded_qvalues_pd[idx][3], linestyle='dotted')\n",
    "            \n",
    "            plt.title(\"Agent \" + str(idx) + \" Prisoner's Dilemma Q-Values in each episode\")\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Q-Value')\n",
    "            plt.legend([\"Defect Given Parter Previously Defected\", \"Cooperate Given Parter Previously Defected\", \n",
    "                        \"Defect Given Parter Previously Cooperated\", \"Cooperate Given Parter Previously Cooperated\"],\n",
    "                        loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "            plt.show()\n",
    "\n",
    "def plot_outcome_changes(results):\n",
    "    recorded_outcome_changes = results[\"recorded_outcome_changes\"]\n",
    "    rounds = results[\"rounds\"]\n",
    "    population = results[\"population\"]\n",
    "    # plot recorded outcome changes\n",
    "    outcome_changes_legend = []\n",
    "    for keys, changes in recorded_outcome_changes.items():\n",
    "        if keys[0] == (ActionPD.COOPERATE, ActionPD.COOPERATE):\n",
    "            style = 'solid'\n",
    "        elif keys[0] == (ActionPD.COOPERATE, ActionPD.DEFECT):\n",
    "            style = 'dotted'\n",
    "        elif keys[0] == (ActionPD.DEFECT, ActionPD.COOPERATE):\n",
    "            style = 'dashed'\n",
    "        elif keys[0] == (ActionPD.DEFECT, ActionPD.DEFECT):\n",
    "            style = 'dashdot'\n",
    "\n",
    "        if keys[1] == (ActionPD.COOPERATE, ActionPD.COOPERATE):\n",
    "            color = 'red'\n",
    "        elif keys[1] == (ActionPD.COOPERATE, ActionPD.DEFECT):\n",
    "            color = 'green'\n",
    "        elif keys[1] == (ActionPD.DEFECT, ActionPD.COOPERATE):\n",
    "            color = 'orange'\n",
    "        elif keys[1] == (ActionPD.DEFECT, ActionPD.DEFECT):\n",
    "            color = 'blue'\n",
    "        \n",
    "        changes = np.divide(changes, rounds * population)\n",
    "\n",
    "        plt.plot(changes, linestyle=style, color=color)\n",
    "        outcome_changes_legend.append(\"(%s, %s) -> (%s, %s)\" % (keys[0][0].name, keys[0][1].name, keys[1][0].name, keys[1][1].name))\n",
    "    \n",
    "\n",
    "\n",
    "    # Plot the changes in outcomes for prisoner's dilemma games per episode\n",
    "    plt.title(\"Prisoner's Dilemma Outcome Changes Per Episode\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Changes')\n",
    "    plt.legend(outcome_changes_legend, loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "The values in the Prisoner's Dilemma Q-Table aproaches the values in the partner selction Q-Table plus one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\.conda\\envs\\evolution12\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\jacob\\.conda\\envs\\evolution12\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[70.44266945 70.53568866]\n",
      " [72.91684356 72.61313536]]\n",
      "[[71.45688886 71.13337897]\n",
      " [73.9324773  70.57582017]]\n",
      "Agent 1) PS-Strategy: Always-Leave, PD-Strategy: Reverse-TFT\n",
      "[[69.95679014 69.82559044]\n",
      " [73.26032705 72.84301565]]\n",
      "[[70.29234808 70.44268885]\n",
      " [70.52414549 70.35810205]]\n",
      "Agent 2) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Cooperate\n",
      "[[ 76.69402883  76.58842901]\n",
      " [ 77.60248895 146.76597651]]\n",
      "[[ 77.10030627  77.16460456]\n",
      " [ 77.16518659 148.27415008]]\n",
      "Agent 3) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[75.66041781 78.89498822]\n",
      " [76.70310869 82.33251178]]\n",
      "[[79.36086819 75.95744357]\n",
      " [82.65039287 75.75508723]]\n",
      "Agent 4) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[67.66580627 67.96449179]\n",
      " [69.83329745 70.26631429]]\n",
      "[[68.55244485 67.87680101]\n",
      " [68.61517312 67.53233256]]\n",
      "Agent 5) PS-Strategy: Always-Stay, PD-Strategy: Always-Cooperate\n",
      "[[70.06645041 70.19145312]\n",
      " [71.67360978 85.42954666]]\n",
      "[[70.89282061 72.18470454]\n",
      " [70.57724649 86.95344556]]\n",
      "Agent 6) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[69.50552952 69.94490345]\n",
      " [70.74363547 70.88553943]]\n",
      "[[70.4761133  69.64454432]\n",
      " [69.99559179 70.35773154]]\n",
      "Agent 7) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 78.84539686  78.53354751]\n",
      " [ 79.8700151  146.84330207]]\n",
      "[[ 79.01484512  78.73961164]\n",
      " [ 79.06911034 148.35145581]]\n",
      "Agent 8) PS-Strategy: Out-For-Tat, PD-Strategy: Reverse-TFT\n",
      "[[70.25911653 69.60515286]\n",
      " [71.45549154 71.67479577]]\n",
      "[[70.11627877 70.23805994]\n",
      " [70.85851272 70.0806074 ]]\n",
      "Agent 9) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[71.35794994 69.78171312]\n",
      " [71.74492958 71.66663938]]\n",
      "[[70.26990669 70.00287186]\n",
      " [71.93305852 70.32631696]]\n",
      "Agent 10) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[67.52088629 68.00323608]\n",
      " [69.78465966 69.32380067]]\n",
      "[[68.13481763 68.11541987]\n",
      " [69.32007345 67.69852386]]\n",
      "Agent 11) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[69.79320434 69.45649705]\n",
      " [70.89445963 70.82465407]]\n",
      "[[69.96808504 69.65008514]\n",
      " [70.33917231 69.90736523]]\n",
      "Agent 12) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Cooperate\n",
      "[[ 72.93578105  72.29318033]\n",
      " [ 73.50407528 125.49960315]]\n",
      "[[ 72.98837555  73.0083742 ]\n",
      " [ 72.81430685 127.01322895]]\n",
      "Agent 13) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[70.41223598 70.31046424]\n",
      " [71.70173769 96.75300365]]\n",
      "[[70.85818301 70.75354599]\n",
      " [70.99262929 98.27399946]]\n",
      "Agent 14) PS-Strategy: Out-For-Tat, PD-Strategy: Reverse-TFT\n",
      "[[68.90237506 68.80476173]\n",
      " [71.43919511 71.48234151]]\n",
      "[[69.32813288 69.35825606]\n",
      " [69.49612882 68.74577625]]\n",
      "Agent 15) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[68.92652125 68.81835227]\n",
      " [70.10054474 84.4624185 ]]\n",
      "[[69.35187912 69.1265877 ]\n",
      " [69.34619252 85.98656535]]\n",
      "Agent 16) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[68.99138258 65.80266041]\n",
      " [67.67430227 70.63990744]]\n",
      "[[66.34365052 66.29842128]\n",
      " [69.54973004 66.14295666]]\n",
      "Agent 17) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 71.75782616  71.19176048]\n",
      " [ 72.40109479 125.12819657]]\n",
      "[[ 71.74604205  71.18469595]\n",
      " [ 71.7094271  126.6419176 ]]\n",
      "Agent 18) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[70.33115061 69.41975091]\n",
      " [71.45141158 71.51381355]]\n",
      "[[69.96114716 69.46893276]\n",
      " [70.9066104  70.01367165]]\n",
      "Agent 19) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[68.47496084 68.35802945]\n",
      " [69.79634512 95.41883434]]\n",
      "[[68.89151497 68.57554763]\n",
      " [68.96685905 96.9401722 ]]\n",
      "Agent 0) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.26045556 66.5365593 ]\n",
      " [67.89549563 68.06440283]]\n",
      "[[67.93605262 66.45036312]\n",
      " [67.9012691  66.96235219]]\n",
      "Agent 1) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[68.33871105 68.77206782]\n",
      " [70.0081543  69.67096809]]\n",
      "[[69.40508122 68.73060835]\n",
      " [68.83334666 68.77074259]]\n",
      "Agent 2) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 72.29882039  72.1583587 ]\n",
      " [ 73.03943627 141.42402074]]\n",
      "[[ 72.72978151  72.06940514]\n",
      " [ 72.74509778 142.93356388]]\n",
      "Agent 3) PS-Strategy: Out-For-Tat, PD-Strategy: Reverse-TFT\n",
      "[[68.03558104 67.89900726]\n",
      " [68.83248683 68.83791064]]\n",
      "[[68.41900038 68.42003285]\n",
      " [68.57525683 68.32787397]]\n",
      "Agent 4) PS-Strategy: Reverse-OFT, PD-Strategy: Tit-For-Tat\n",
      "[[66.31785128 66.4297654 ]\n",
      " [68.05684148 67.99944813]]\n",
      "[[66.94113355 66.90608155]\n",
      " [66.84950906 66.85510827]]\n",
      "Agent 5) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[65.69832727 66.15540774]\n",
      " [67.23937995 67.07618364]]\n",
      "[[66.21443407 66.18560727]\n",
      " [66.97420249 65.60813512]]\n",
      "Agent 6) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[70.54163523 69.57628325]\n",
      " [71.22640804 71.66763181]]\n",
      "[[71.27795601 69.76306663]\n",
      " [71.04553264 69.90567938]]\n",
      "Agent 7) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[66.01796491 66.12314697]\n",
      " [67.71669751 67.70061567]]\n",
      "[[66.63321868 66.59433954]\n",
      " [66.54410557 66.30193874]]\n",
      "Agent 8) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[68.35376302 68.10250648]\n",
      " [69.74751226 69.68640984]]\n",
      "[[69.34165956 68.56254139]\n",
      " [69.00713404 68.50807093]]\n",
      "Agent 9) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 72.97634061  72.77343215]\n",
      " [ 73.65393783 141.47565687]]\n",
      "[[ 73.53798746  73.26400573]\n",
      " [ 73.31812994 142.98518677]]\n",
      "Agent 10) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.09684009 65.68337893]\n",
      " [67.17396388 67.18048889]]\n",
      "[[66.73168826 66.2034624 ]\n",
      " [67.02962625 65.89536738]]\n",
      "Agent 11) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[65.55710406 65.53530845]\n",
      " [67.48987919 67.36976341]]\n",
      "[[66.07001954 66.04279646]\n",
      " [66.08312525 65.96959447]]\n",
      "Agent 12) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[70.0331087  73.03080019]\n",
      " [71.69486959 76.4676702 ]]\n",
      "[[73.66736228 70.44614012]\n",
      " [76.1022177  70.5767505 ]]\n",
      "Agent 13) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 80.29095037  80.29772541]\n",
      " [ 81.02251023 148.85093658]]\n",
      "[[ 81.0198694   80.99764035]\n",
      " [ 80.78535682 150.35857562]]\n",
      "Agent 14) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[67.25551301 67.25674686]\n",
      " [68.69367489 68.73217889]]\n",
      "[[67.7845256  67.55140526]\n",
      " [67.77813449 67.69615154]]\n",
      "Agent 15) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[67.54556692 67.42695177]\n",
      " [69.04123457 68.98653184]]\n",
      "[[67.91893401 67.57004518]\n",
      " [68.15097803 67.62910342]]\n",
      "Agent 16) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 81.49889552  81.47763328]\n",
      " [ 82.42457644 148.85856875]]\n",
      "[[ 82.07199764  81.92132364]\n",
      " [ 82.04901007 150.36620583]]\n",
      "Agent 17) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[69.60180359 69.40050328]\n",
      " [71.28420767 71.38246228]]\n",
      "[[69.95481883 69.8835778 ]\n",
      " [70.19048333 69.93502332]]\n",
      "Agent 18) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.41846403 66.35537041]\n",
      " [68.20749296 68.19570917]]\n",
      "[[66.88249896 66.52538357]\n",
      " [66.94509683 66.52152575]]\n",
      "Agent 19) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[68.6171552  68.28424951]\n",
      " [69.83076708 70.00693778]]\n",
      "[[69.70287268 68.78198088]\n",
      " [69.08224811 68.68751604]]\n",
      "Agent 0) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.49794316 67.18153839]\n",
      " [68.17973998 68.31292347]]\n",
      "[[68.68561717 67.45974467]\n",
      " [68.30725717 67.32777425]]\n",
      "Agent 1) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[66.83587486 66.94301057]\n",
      " [68.48130462 68.29700825]]\n",
      "[[67.6394302  67.51265422]\n",
      " [67.74641837 66.77541804]]\n",
      "Agent 2) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[67.4870781  67.48368392]\n",
      " [68.91626731 68.76098942]]\n",
      "[[68.1525048  67.5256964 ]\n",
      " [68.45706402 67.9036525 ]]\n",
      "Agent 3) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[69.3502791  69.08495684]\n",
      " [70.30870881 70.16093303]]\n",
      "[[70.24333049 69.57862878]\n",
      " [70.92536957 69.17435949]]\n",
      "Agent 4) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[65.30299909 65.3278396 ]\n",
      " [66.28103475 66.28830028]]\n",
      "[[65.86040092 65.75520839]\n",
      " [66.22051368 65.87631226]]\n",
      "Agent 5) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[68.27783309 69.79860121]\n",
      " [69.81799326 71.79112285]]\n",
      "[[70.58758002 68.69515374]\n",
      " [70.19471412 68.53192202]]\n",
      "Agent 6) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.60295452 67.267329  ]\n",
      " [68.5444888  68.69833364]]\n",
      "[[68.07173791 67.71156801]\n",
      " [68.35346205 67.38689414]]\n",
      "Agent 7) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.7198713  67.13178125]\n",
      " [68.4309051  68.60948473]]\n",
      "[[68.38397084 67.59826034]\n",
      " [68.50769667 67.32867414]]\n",
      "Agent 8) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.26762987 66.07571657]\n",
      " [67.09261593 66.93803476]]\n",
      "[[67.63349341 66.63761251]\n",
      " [67.46713543 66.42136228]]\n",
      "Agent 9) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.24579075 66.15999491]\n",
      " [67.79499258 67.83421732]]\n",
      "[[66.66658378 66.66133439]\n",
      " [66.96656444 66.30859987]]\n",
      "Agent 10) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[65.99391278 66.20321117]\n",
      " [67.97324993 67.88112005]]\n",
      "[[66.83486357 66.444017  ]\n",
      " [66.47734657 66.11978969]]\n",
      "Agent 11) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[65.26233175 64.93397108]\n",
      " [66.24322759 66.09001483]]\n",
      "[[66.62552194 65.02698806]\n",
      " [65.9550964  65.18547415]]\n",
      "Agent 12) PS-Strategy: Always-Leave, PD-Strategy: Reverse-TFT\n",
      "[[66.65531754 66.52381898]\n",
      " [67.76702641 67.73476147]]\n",
      "[[67.05578026 67.06356321]\n",
      " [67.21258792 67.1647838 ]]\n",
      "Agent 13) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[68.64159016 66.72651264]\n",
      " [68.72232282 69.56805517]]\n",
      "[[69.06040746 67.16922377]\n",
      " [69.7060704  67.23426363]]\n",
      "Agent 14) PS-Strategy: Always-Stay, PD-Strategy: Reverse-TFT\n",
      "[[66.99492357 67.00980243]\n",
      " [68.21411868 68.26976129]]\n",
      "[[67.49810844 67.52018671]\n",
      " [67.61652714 67.2855533 ]]\n",
      "Agent 15) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.1279227  65.81822116]\n",
      " [67.08948878 67.06723664]]\n",
      "[[66.58730528 66.276378  ]\n",
      " [66.85996583 66.16886575]]\n",
      "Agent 16) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[66.54487923 66.63139482]\n",
      " [68.29114942 68.23164656]]\n",
      "[[67.09321599 66.60643037]\n",
      " [67.17602448 66.77178543]]\n",
      "Agent 17) PS-Strategy: Always-Leave, PD-Strategy: Reverse-TFT\n",
      "[[66.2027683  66.09143087]\n",
      " [67.6610691  67.5131403 ]]\n",
      "[[66.66205593 66.7379934 ]\n",
      " [67.20093749 66.43695504]]\n",
      "Agent 18) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[65.91657325 65.95434341]\n",
      " [67.61816098 67.48572653]]\n",
      "[[66.45310523 66.21194372]\n",
      " [66.52352071 66.10189768]]\n",
      "Agent 19) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.53390464 67.1461077 ]\n",
      " [68.50894205 68.55697106]]\n",
      "[[68.84470252 67.25965627]\n",
      " [67.99646749 67.45194263]]\n",
      "Agent 0) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 71.00502315  71.34070378]\n",
      " [ 72.16492541 139.07357258]]\n",
      "[[ 71.62498422  71.38679814]\n",
      " [ 71.5871492  140.58371832]]\n",
      "Agent 1) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.49078204 65.16818428]\n",
      " [66.2161907  66.22683575]]\n",
      "[[65.92636896 65.64924007]\n",
      " [66.11267193 65.83397382]]\n",
      "Agent 2) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[68.3360045  69.02270179]\n",
      " [69.88953614 70.9825032 ]]\n",
      "[[69.31890597 68.69099718]\n",
      " [70.14407669 68.65533119]]\n",
      "Agent 3) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.03719101 65.84571161]\n",
      " [67.09626287 67.24865812]]\n",
      "[[66.76774243 66.32048556]\n",
      " [66.96314595 66.19778647]]\n",
      "Agent 4) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[66.94433196 66.95256385]\n",
      " [68.51796749 68.48006211]]\n",
      "[[67.8881125  66.82211972]\n",
      " [67.63475465 67.21815438]]\n",
      "Agent 5) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.28751139 66.22369271]\n",
      " [67.14018749 67.25328462]]\n",
      "[[67.6499451  66.60672824]\n",
      " [67.94748724 66.71013728]]\n",
      "Agent 6) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[65.51144932 65.56733855]\n",
      " [67.07682124 67.10168097]]\n",
      "[[66.04139029 65.94721813]\n",
      " [66.06144806 65.6643219 ]]\n",
      "Agent 7) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 70.51926099  69.96611113]\n",
      " [ 71.37822759 138.94941389]]\n",
      "[[ 70.64968477  69.84450839]\n",
      " [ 70.52436103 140.45959146]]\n",
      "Agent 8) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.99729229 66.87362877]\n",
      " [67.91719047 68.57928404]]\n",
      "[[68.74775532 67.11628563]\n",
      " [68.73650575 66.99483765]]\n",
      "Agent 9) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.46905079 66.37098438]\n",
      " [68.34967974 68.40787211]]\n",
      "[[66.8925629  66.81671227]\n",
      " [66.99488965 66.48662407]]\n",
      "Agent 10) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.4457885  66.124885  ]\n",
      " [66.97167238 66.94324077]]\n",
      "[[66.97386856 66.48516225]\n",
      " [67.03079086 66.45279554]]\n",
      "Agent 11) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[64.7142906  64.63588557]\n",
      " [66.2150036  66.19797787]]\n",
      "[[65.36364247 64.72435012]\n",
      " [65.45712137 65.20811174]]\n",
      "Agent 12) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[68.83378714 68.79572373]\n",
      " [70.27467692 70.24001488]]\n",
      "[[69.24046231 68.95624414]\n",
      " [69.57295527 69.39695675]]\n",
      "Agent 13) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[64.46557274 64.3804242 ]\n",
      " [66.44427139 66.38810807]]\n",
      "[[64.88750898 64.59827901]\n",
      " [65.06471417 64.58048497]]\n",
      "Agent 14) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.58877814 65.8221711 ]\n",
      " [66.87257881 66.87062158]]\n",
      "[[67.13979616 65.73433546]\n",
      " [67.38664584 66.26616844]]\n",
      "Agent 15) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[67.17779191 67.05352748]\n",
      " [67.91366288 67.90946379]]\n",
      "[[67.79452314 67.34754084]\n",
      " [67.73332462 67.55807681]]\n",
      "Agent 16) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[69.76622415 69.29221904]\n",
      " [71.02073831 70.98625226]]\n",
      "[[70.37899881 69.69338581]\n",
      " [70.30622786 69.66522196]]\n",
      "Agent 17) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[67.59673997 67.51813625]\n",
      " [69.02992233 68.98718866]]\n",
      "[[69.14362285 67.73526336]\n",
      " [67.99915644 67.84291689]]\n",
      "Agent 18) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.70244257 65.09084656]\n",
      " [66.64744842 67.51409392]]\n",
      "[[67.33783667 65.53969035]\n",
      " [67.24744074 65.56169621]]\n",
      "Agent 19) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.72557147 65.68056432]\n",
      " [66.7258193  66.81273599]]\n",
      "[[66.45994196 65.79358815]\n",
      " [66.24230243 66.10720934]]\n",
      "Agent 0) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.27645024 65.21638602]\n",
      " [66.38859831 66.45632024]]\n",
      "[[65.86706186 65.21904167]\n",
      " [65.8009902  65.73202915]]\n",
      "Agent 1) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.2376079  66.13034323]\n",
      " [67.67100851 67.68487059]]\n",
      "[[66.85968717 66.45047494]\n",
      " [66.74457223 66.48904827]]\n",
      "Agent 2) PS-Strategy: Always-Leave, PD-Strategy: Reverse-TFT\n",
      "[[67.08495624 66.95948954]\n",
      " [68.88223276 68.63218919]]\n",
      "[[67.48524715 67.50418714]\n",
      " [67.77124917 67.49508225]]\n",
      "Agent 3) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[68.14553352 67.105062  ]\n",
      " [68.69796485 68.7900048 ]]\n",
      "[[68.67822292 67.32125778]\n",
      " [68.8540986  67.35678642]]\n",
      "Agent 4) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.82064716 66.19737714]\n",
      " [67.68398321 68.1158244 ]]\n",
      "[[67.38893942 66.48605392]\n",
      " [67.41056675 66.57505645]]\n",
      "Agent 5) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.19296907 66.19110261]\n",
      " [68.02261694 68.07517124]]\n",
      "[[67.78012837 66.67651214]\n",
      " [67.73119111 66.68295641]]\n",
      "Agent 6) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.25755594 66.21411883]\n",
      " [68.09404368 68.02962556]]\n",
      "[[66.96190037 66.30163331]\n",
      " [66.85028911 66.49483403]]\n",
      "Agent 7) PS-Strategy: Always-Leave, PD-Strategy: Tit-For-Tat\n",
      "[[64.93744459 64.88582576]\n",
      " [66.5885605  66.54881758]]\n",
      "[[65.42467933 65.40854164]\n",
      " [65.41329842 65.45124087]]\n",
      "Agent 8) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.35900386 66.25319671]\n",
      " [67.98874665 68.07668656]]\n",
      "[[67.00111546 66.13070418]\n",
      " [67.02690078 66.32170058]]\n",
      "Agent 9) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[64.60543286 64.47528815]\n",
      " [66.66834853 66.6644952 ]]\n",
      "[[65.00862906 64.93799587]\n",
      " [65.18747854 64.82568303]]\n",
      "Agent 10) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.74562285 67.61179127]\n",
      " [68.79551076 68.83394113]]\n",
      "[[68.14283227 67.97584105]\n",
      " [68.32095204 68.01396935]]\n",
      "Agent 11) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[65.63156617 65.60093596]\n",
      " [67.40010564 67.3478921 ]]\n",
      "[[66.17035383 65.8664148 ]\n",
      " [66.17753389 65.96905155]]\n",
      "Agent 12) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.87424852 65.90769575]\n",
      " [66.84571268 67.40503029]]\n",
      "[[67.65796855 66.3343114 ]\n",
      " [67.6013079  66.23147578]]\n",
      "Agent 13) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.73911406 66.6400911 ]\n",
      " [68.59106444 68.75422447]]\n",
      "[[67.18263909 67.00781006]\n",
      " [67.23419071 67.21068806]]\n",
      "Agent 14) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[65.85439567 65.87306387]\n",
      " [67.28944252 67.1286857 ]]\n",
      "[[66.51774199 66.08792076]\n",
      " [66.29803562 66.29385868]]\n",
      "Agent 15) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[66.76560222 66.81934404]\n",
      " [68.06356724 68.08137823]]\n",
      "[[67.35045462 67.0335533 ]\n",
      " [67.31296697 67.01512795]]\n",
      "Agent 16) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.07826992 66.0124696 ]\n",
      " [67.33995006 67.32532255]]\n",
      "[[66.78525764 66.43578489]\n",
      " [66.58057432 66.51075466]]\n",
      "Agent 17) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[69.83777559 67.96423932]\n",
      " [68.82327824 70.63480454]]\n",
      "[[70.39605556 68.00255027]\n",
      " [70.62743713 68.37664476]]\n",
      "Agent 18) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[66.37062201 66.28272723]\n",
      " [67.79231122 99.97804059]]\n",
      "[[ 67.31199946  66.79087247]\n",
      " [ 66.96761103 101.49820957]]\n",
      "Agent 19) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 67.42266147  67.51177926]\n",
      " [ 68.82361569 100.51188083]]\n",
      "[[ 68.22645552  67.8640502 ]\n",
      " [ 68.02716201 102.03191294]]\n",
      "Agent 0) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.02639683 65.93057845]\n",
      " [66.86202828 66.85976966]]\n",
      "[[66.55136548 66.04574414]\n",
      " [66.62281989 66.58229651]]\n",
      "Agent 1) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.82184726 65.81944844]\n",
      " [67.10056339 67.13615445]]\n",
      "[[66.32186534 65.91280746]\n",
      " [66.47284132 66.31394987]]\n",
      "Agent 2) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.00699211 65.95828377]\n",
      " [67.56568805 67.5215217 ]]\n",
      "[[66.63367671 66.01815942]\n",
      " [66.89455534 66.41064715]]\n",
      "Agent 3) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.53746637 66.53123649]\n",
      " [68.10683904 68.06405109]]\n",
      "[[66.94639893 66.93087245]\n",
      " [67.25645232 67.11138891]]\n",
      "Agent 4) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.39401201 65.3316968 ]\n",
      " [66.59957438 66.69539425]]\n",
      "[[66.02694242 65.80745169]\n",
      " [65.88444802 65.75421348]]\n",
      "Agent 5) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[68.07908999 67.96504242]\n",
      " [68.9364389  68.9364035 ]]\n",
      "[[68.54259    67.85003815]\n",
      " [68.86154063 68.67982364]]\n",
      "Agent 6) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[66.62884722 66.70442091]\n",
      " [68.93656047 68.98841098]]\n",
      "[[67.10769906 66.75757329]\n",
      " [67.59884488 67.3061609 ]]\n",
      "Agent 7) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.1927073  65.1857806 ]\n",
      " [66.33865588 66.39467659]]\n",
      "[[65.66714649 65.41603053]\n",
      " [65.99260114 65.61762111]]\n",
      "Agent 8) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.10869817 65.61124459]\n",
      " [66.84877226 66.81057922]]\n",
      "[[66.60042647 65.85509511]\n",
      " [66.81725509 66.06920551]]\n",
      "Agent 9) PS-Strategy: Always-Leave, PD-Strategy: Tit-For-Tat\n",
      "[[65.7309628  65.71998707]\n",
      " [67.60596133 67.59851568]]\n",
      "[[66.19729185 65.91961814]\n",
      " [66.3465811  66.41898255]]\n",
      "Agent 10) PS-Strategy: Always-Leave, PD-Strategy: Tit-For-Tat\n",
      "[[67.06459669 67.00359176]\n",
      " [68.28251345 68.26158728]]\n",
      "[[67.59480889 66.86838184]\n",
      " [67.67392166 67.6751138 ]]\n",
      "Agent 11) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.67854929 66.59217022]\n",
      " [67.42289395 67.43264785]]\n",
      "[[67.49163848 67.05795124]\n",
      " [67.37648947 67.35218175]]\n",
      "Agent 12) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[65.67328375 65.61999583]\n",
      " [66.85154523 76.27706431]]\n",
      "[[66.09931215 66.03538627]\n",
      " [66.31444096 77.8033097 ]]\n",
      "Agent 13) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[64.41748411 64.20972287]\n",
      " [66.0054918  66.14554467]]\n",
      "[[65.07826761 64.41158198]\n",
      " [64.94182199 64.52414409]]\n",
      "Agent 14) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[66.14821559 66.22296203]\n",
      " [67.29510947 67.22530071]]\n",
      "[[66.77855563 66.23962918]\n",
      " [66.66864997 66.33109291]]\n",
      "Agent 15) PS-Strategy: Reverse-OFT, PD-Strategy: Tit-For-Tat\n",
      "[[67.19775353 67.29155366]\n",
      " [68.37460018 68.2648892 ]]\n",
      "[[67.74643116 67.63142979]\n",
      " [67.79221702 67.79523162]]\n",
      "Agent 16) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.44069034 66.405497  ]\n",
      " [67.37772426 67.91727802]]\n",
      "[[67.20533189 66.26057187]\n",
      " [70.03894669 66.90607221]]\n",
      "Agent 17) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.44272528 65.31381459]\n",
      " [66.68401516 66.89634572]]\n",
      "[[66.68177319 65.41551535]\n",
      " [67.60616277 65.85773463]]\n",
      "Agent 18) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[69.24984047 67.23307723]\n",
      " [68.43463837 69.64877483]]\n",
      "[[70.04983344 67.58336057]\n",
      " [70.06425519 67.70573369]]\n",
      "Agent 19) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[66.97888583 66.99140599]\n",
      " [67.96567706 77.29468974]]\n",
      "[[67.73377771 67.09666559]\n",
      " [67.50062888 78.82067423]]\n",
      "Agent 0) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[65.58705352 65.52007623]\n",
      " [68.40337813 68.28187266]]\n",
      "[[66.2545918  65.85649237]\n",
      " [66.19415192 66.17873283]]\n",
      "Agent 1) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 73.93805405  74.0115321 ]\n",
      " [ 74.75787166 146.0521368 ]]\n",
      "[[ 74.64971117  74.41609668]\n",
      " [ 74.65515716 147.56049339]]\n",
      "Agent 2) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.90128719 66.48003036]\n",
      " [67.59541273 68.54684905]]\n",
      "[[68.49546354 66.84395451]\n",
      " [68.59625919 66.90851267]]\n",
      "Agent 3) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.19855276 66.77983563]\n",
      " [67.97923645 67.99580569]]\n",
      "[[68.43602514 67.07270198]\n",
      " [67.64288186 67.22838023]]\n",
      "Agent 4) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.12765106 65.36587385]\n",
      " [67.15372369 67.01426168]]\n",
      "[[67.16225525 65.57620944]\n",
      " [66.59075218 65.93590296]]\n",
      "Agent 5) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[64.7938731  64.71723735]\n",
      " [65.71462662 65.8139682 ]]\n",
      "[[65.66056895 64.55853821]\n",
      " [65.47832879 65.06239739]]\n",
      "Agent 6) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.30317662 65.62685518]\n",
      " [66.78085419 66.91499289]]\n",
      "[[66.91513063 65.6675693 ]\n",
      " [67.63592024 65.92954275]]\n",
      "Agent 7) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 73.93131995  73.93787182]\n",
      " [ 74.85989212 146.05474252]]\n",
      "[[ 74.61745053  74.17225948]\n",
      " [ 74.58747388 147.56309844]]\n",
      "Agent 8) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[65.2523834  65.29320063]\n",
      " [66.59472216 66.64635408]]\n",
      "[[65.77119513 65.37866613]\n",
      " [65.96771575 65.83747243]]\n",
      "Agent 9) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[64.95089202 64.82262416]\n",
      " [67.2193507  67.06015106]]\n",
      "[[65.35502262 64.97548765]\n",
      " [65.52440452 65.48522305]]\n",
      "Agent 10) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[65.13396274 65.17996774]\n",
      " [66.82492244 66.83799091]]\n",
      "[[65.63077413 65.21030048]\n",
      " [65.78841149 65.73774945]]\n",
      "Agent 11) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.28653947 65.16424612]\n",
      " [66.97452578 67.02297748]]\n",
      "[[65.69299753 65.3311888 ]\n",
      " [65.85875198 65.83501843]]\n",
      "Agent 12) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[63.95296304 63.76846922]\n",
      " [65.42248993 65.56154507]]\n",
      "[[64.98415973 63.88080262]\n",
      " [64.4414919  64.08924018]]\n",
      "Agent 13) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[70.09920699 68.060663  ]\n",
      " [69.0408821  70.9961089 ]]\n",
      "[[70.72004466 68.35322693]\n",
      " [70.66345811 68.83594883]]\n",
      "Agent 14) PS-Strategy: Always-Leave, PD-Strategy: Tit-For-Tat\n",
      "[[66.03772589 65.95852294]\n",
      " [67.37063101 67.15286466]]\n",
      "[[67.09386878 66.37244722]\n",
      " [66.57351824 66.69687239]]\n",
      "Agent 15) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.3580803  65.81258273]\n",
      " [67.892246   68.2060388 ]]\n",
      "[[68.17045409 66.02964278]\n",
      " [68.05784165 66.32820344]]\n",
      "Agent 16) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.48875227 66.44145466]\n",
      " [67.71781278 67.76103106]]\n",
      "[[66.92671798 66.55791612]\n",
      " [67.1920096  66.7966194 ]]\n",
      "Agent 17) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.87635437 66.03408398]\n",
      " [68.00197256 68.09498517]]\n",
      "[[67.17035154 66.14377502]\n",
      " [68.07993122 66.60292397]]\n",
      "Agent 18) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.80160537 66.33616957]\n",
      " [67.62871845 67.56764002]]\n",
      "[[67.59270043 66.76088723]\n",
      " [67.7749261  66.71181472]]\n",
      "Agent 19) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[67.28695866 67.36283781]\n",
      " [68.70452668 68.6475255 ]]\n",
      "[[68.03775532 67.52322647]\n",
      " [67.92824192 67.87921799]]\n",
      "Agent 0) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[65.65096831 66.23256517]\n",
      " [66.9958739  67.27794743]]\n",
      "[[66.16409576 66.04806939]\n",
      " [68.08067205 66.12708995]]\n",
      "Agent 1) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[68.78297216 68.77455416]\n",
      " [70.08279325 70.15501417]]\n",
      "[[69.28883442 69.07395425]\n",
      " [69.51371518 69.3309575 ]]\n",
      "Agent 2) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[64.62385346 64.54798348]\n",
      " [65.95760012 66.00912354]]\n",
      "[[65.12929386 64.7642808 ]\n",
      " [65.25744716 65.15467316]]\n",
      "Agent 3) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.00122051 66.57693021]\n",
      " [67.52483282 67.66581737]]\n",
      "[[67.54788629 67.01021106]\n",
      " [67.61353389 67.08818881]]\n",
      "Agent 4) PS-Strategy: Always-Leave, PD-Strategy: Tit-For-Tat\n",
      "[[65.69533946 65.68943209]\n",
      " [67.25157006 67.20372683]]\n",
      "[[66.31850579 66.01997003]\n",
      " [66.19963228 66.22383006]]\n",
      "Agent 5) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.81550507 66.80987188]\n",
      " [69.17562659 69.07728455]]\n",
      "[[67.34291801 67.24870506]\n",
      " [67.55648517 67.15644489]]\n",
      "Agent 6) PS-Strategy: Always-Stay, PD-Strategy: Reverse-TFT\n",
      "[[67.00718403 67.080508  ]\n",
      " [68.5751567  68.68388157]]\n",
      "[[67.53596783 67.54805116]\n",
      " [67.60631291 67.42383545]]\n",
      "Agent 7) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[65.69232395 65.73610791]\n",
      " [67.84617549 67.79742271]]\n",
      "[[66.17245469 65.96823543]\n",
      " [66.72525149 66.71924126]]\n",
      "Agent 8) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[65.76964156 65.5025443 ]\n",
      " [67.36600913 67.44292372]]\n",
      "[[66.36094028 65.84116702]\n",
      " [66.26788662 66.29270129]]\n",
      "Agent 9) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.63512696 66.61656298]\n",
      " [68.62950698 68.6288115 ]]\n",
      "[[67.07959692 66.6514156 ]\n",
      " [67.47830385 67.47365571]]\n",
      "Agent 10) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[69.26920287 67.96015502]\n",
      " [69.12496581 70.44580376]]\n",
      "[[69.71925281 68.17327552]\n",
      " [70.0516739  68.56589182]]\n",
      "Agent 11) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[67.72307366 67.75441189]\n",
      " [68.83669743 68.75815742]]\n",
      "[[68.22877359 68.0484158 ]\n",
      " [68.58206841 68.29107116]]\n",
      "Agent 12) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.30677613 65.00476535]\n",
      " [66.8139617  67.11502315]]\n",
      "[[65.81340742 65.4319514 ]\n",
      " [65.92372274 65.71261459]]\n",
      "Agent 13) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[67.15823528 67.01916685]\n",
      " [68.85946654 68.87273633]]\n",
      "[[67.55184195 67.17421489]\n",
      " [67.99623666 68.04509568]]\n",
      "Agent 14) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.36694766 65.28099969]\n",
      " [66.73421982 67.21956331]]\n",
      "[[66.87810753 65.54659608]\n",
      " [67.08770677 65.53409819]]\n",
      "Agent 15) PS-Strategy: Reverse-OFT, PD-Strategy: Reverse-TFT\n",
      "[[64.14401171 64.16855743]\n",
      " [66.66672473 66.40472454]]\n",
      "[[64.63344179 64.63856959]\n",
      " [64.8998297  64.61990802]]\n",
      "Agent 16) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.45961367 66.40073876]\n",
      " [67.47175306 67.93212681]]\n",
      "[[67.86464089 66.68613993]\n",
      " [68.31346172 67.00178586]]\n",
      "Agent 17) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[64.53087486 64.50905952]\n",
      " [66.19580261 66.18537744]]\n",
      "[[64.96034387 64.61967329]\n",
      " [65.37428551 65.16501305]]\n",
      "Agent 18) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.73837014 65.58517466]\n",
      " [67.68717118 67.73403124]]\n",
      "[[66.13085599 65.65746624]\n",
      " [66.78470189 66.11959286]]\n",
      "Agent 19) PS-Strategy: Always-Stay, PD-Strategy: Reverse-TFT\n",
      "[[65.59340935 65.66356341]\n",
      " [67.19476829 67.23679208]]\n",
      "[[66.08504079 66.10331333]\n",
      " [66.35433352 65.98061791]]\n",
      "Agent 0) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[63.54249711 63.53930221]\n",
      " [65.37298044 65.5323541 ]]\n",
      "[[64.39539646 63.56796924]\n",
      " [64.26395244 64.27617971]]\n",
      "Agent 1) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[68.34890168 67.96067897]\n",
      " [69.42195137 69.44581463]]\n",
      "[[68.88876025 67.82526975]\n",
      " [69.21369112 68.87920315]]\n",
      "Agent 2) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.29964814 66.0898844 ]\n",
      " [67.44622415 67.6602204 ]]\n",
      "[[67.26377357 65.93182763]\n",
      " [67.32850302 66.75858748]]\n",
      "Agent 3) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 80.25648304  80.15488228]\n",
      " [ 81.19796492 148.6316386 ]]\n",
      "[[ 81.12498467  80.42692126]\n",
      " [ 80.99925747 150.13933386]]\n",
      "Agent 4) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.37789706 66.05132033]\n",
      " [69.42308605 69.08634299]]\n",
      "[[67.16776942 66.27023164]\n",
      " [68.83305039 66.58444549]]\n",
      "Agent 5) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[65.68219218 65.58274489]\n",
      " [67.22510823 67.10739189]]\n",
      "[[66.17529421 65.55779564]\n",
      " [66.43414858 66.19300904]]\n",
      "Agent 6) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[65.79217127 65.64264762]\n",
      " [66.70273781 66.65007169]]\n",
      "[[66.66475873 65.53910446]\n",
      " [66.50452001 66.2080877 ]]\n",
      "Agent 7) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[68.65156035 68.50237669]\n",
      " [69.96731492 69.91379073]]\n",
      "[[69.38997714 69.02092765]\n",
      " [69.578795   69.20991447]]\n",
      "Agent 8) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[65.71168129 65.79234335]\n",
      " [66.81931482 66.84851521]]\n",
      "[[66.22551827 66.10494614]\n",
      " [66.58238168 66.5409533 ]]\n",
      "Agent 9) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[65.89961107 66.0006334 ]\n",
      " [67.86880219 67.6399294 ]]\n",
      "[[66.58579334 66.39752391]\n",
      " [67.17730549 66.67147148]]\n",
      "Agent 10) PS-Strategy: Always-Stay, PD-Strategy: Always-Cooperate\n",
      "[[ 79.39205171  79.40671497]\n",
      " [ 80.8317231  148.62803574]]\n",
      "[[ 79.76205176  79.92229278]\n",
      " [ 80.6978051  150.13573192]]\n",
      "Agent 11) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.64681125 66.62299381]\n",
      " [68.23614283 68.26599975]]\n",
      "[[67.40671904 66.8648911 ]\n",
      " [67.47321962 67.32702223]]\n",
      "Agent 12) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.10688178 66.1012471 ]\n",
      " [67.57886655 67.6129296 ]]\n",
      "[[66.61860102 66.41872154]\n",
      " [66.78270165 66.63757232]]\n",
      "Agent 13) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[64.79562757 64.77841414]\n",
      " [66.53074601 66.6355142 ]]\n",
      "[[65.28650375 64.84524836]\n",
      " [65.53676207 65.37999637]]\n",
      "Agent 14) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.2501095  65.6576278 ]\n",
      " [67.35943381 67.39363584]]\n",
      "[[67.19737447 65.8127456 ]\n",
      " [68.91896688 66.27619461]]\n",
      "Agent 15) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[65.43229505 65.3634848 ]\n",
      " [66.82164281 66.74061685]]\n",
      "[[65.81525282 65.7596727 ]\n",
      " [66.65756921 66.21129861]]\n",
      "Agent 16) PS-Strategy: Reverse-OFT, PD-Strategy: Always-Defect\n",
      "[[65.03063692 65.07369399]\n",
      " [67.26631317 67.15420833]]\n",
      "[[65.49040694 65.28935905]\n",
      " [65.92715277 65.9050553 ]]\n",
      "Agent 17) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[66.89128035 66.94387386]\n",
      " [69.0122344  69.10114631]]\n",
      "[[67.56439519 67.16863271]\n",
      " [68.15632764 67.68151525]]\n",
      "Agent 18) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.63465508 66.48993323]\n",
      " [68.8623991  68.6264769 ]]\n",
      "[[67.46370813 66.68614491]\n",
      " [67.61668837 67.48717815]]\n",
      "Agent 19) PS-Strategy: Always-Leave, PD-Strategy: Tit-For-Tat\n",
      "[[64.3269418  64.04902649]\n",
      " [65.45388902 65.3210667 ]]\n",
      "[[65.29571856 64.15439386]\n",
      " [64.92608575 64.99235925]]\n",
      "Agent 0) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.3131128  67.20056154]\n",
      " [69.27456794 69.29887925]]\n",
      "[[67.83137989 67.61478351]\n",
      " [68.4706943  68.47035147]]\n",
      "Agent 1) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.77140009 65.61603776]\n",
      " [67.02770988 67.06024567]]\n",
      "[[67.32997008 65.61518798]\n",
      " [67.94179308 66.90118982]]\n",
      "Agent 2) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.34045917 65.29548779]\n",
      " [68.05447382 68.12270308]]\n",
      "[[65.84185207 65.52396571]\n",
      " [66.55154605 66.23166304]]\n",
      "Agent 3) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.62377125 66.51170492]\n",
      " [68.62278022 68.54890624]]\n",
      "[[67.31476691 66.95477982]\n",
      " [67.6231323  67.59674668]]\n",
      "Agent 4) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.69913467 64.92464689]\n",
      " [66.51606958 66.59534784]]\n",
      "[[66.19780926 65.36323917]\n",
      " [66.77732666 65.830502  ]]\n",
      "Agent 5) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[64.95503566 64.80737258]\n",
      " [66.92220664 66.8479237 ]]\n",
      "[[65.60685174 65.19176233]\n",
      " [65.60417282 65.56758502]]\n",
      "Agent 6) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[66.32606335 66.06021012]\n",
      " [67.76225491 67.78536554]]\n",
      "[[66.9319966  66.46131866]\n",
      " [67.06950378 67.08240403]]\n",
      "Agent 7) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.78477072 66.11038623]\n",
      " [67.5688764  67.60281641]]\n",
      "[[67.31995367 66.55354484]\n",
      " [67.72166798 67.28701221]]\n",
      "Agent 8) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[70.47931768 68.07978153]\n",
      " [70.21017616 72.06000796]]\n",
      "[[70.85424781 68.25183595]\n",
      " [71.93421822 69.86483653]]\n",
      "Agent 9) PS-Strategy: Always-Leave, PD-Strategy: Tit-For-Tat\n",
      "[[66.0622157  66.01453217]\n",
      " [67.453107   67.40776825]]\n",
      "[[66.55262583 65.99301708]\n",
      " [67.21813912 68.6396162 ]]\n",
      "Agent 10) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.27408175 64.86340611]\n",
      " [66.72196322 66.72331608]]\n",
      "[[65.77625924 64.94686381]\n",
      " [66.09784904 66.02740882]]\n",
      "Agent 11) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[64.06704125 64.01660166]\n",
      " [66.85577552 67.2249429 ]]\n",
      "[[64.41458461 64.37165588]\n",
      " [65.26202189 65.02692254]]\n",
      "Agent 12) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[64.8851997  64.71869586]\n",
      " [66.45920546 66.44375544]]\n",
      "[[65.65454137 64.8360151 ]\n",
      " [65.87970179 65.65690631]]\n",
      "Agent 13) PS-Strategy: Always-Leave, PD-Strategy: Tit-For-Tat\n",
      "[[64.84586291 64.79097709]\n",
      " [66.873974   66.82315628]]\n",
      "[[65.41257336 64.74632593]\n",
      " [65.6927958  65.80367068]]\n",
      "Agent 14) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[63.6444284  63.42656193]\n",
      " [65.2245664  65.25317952]]\n",
      "[[64.2178237  63.66602711]\n",
      " [64.22898945 64.15521626]]\n",
      "Agent 15) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[65.47095078 64.93303831]\n",
      " [66.62474837 66.59195986]]\n",
      "[[66.22749626 65.43070872]\n",
      " [66.68418041 66.35486439]]\n",
      "Agent 16) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.90846583 65.75719902]\n",
      " [67.89774112 67.9762358 ]]\n",
      "[[66.44229563 66.15764644]\n",
      " [67.11165217 67.01787685]]\n",
      "Agent 17) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[65.85972749 65.97269633]\n",
      " [68.01966211 68.22052706]]\n",
      "[[66.4552444 66.1936364]\n",
      " [67.4047119 66.8712006]]\n",
      "Agent 18) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[66.50702357 65.43498525]\n",
      " [67.26914279 67.40434704]]\n",
      "[[67.24144396 65.68587973]\n",
      " [66.73430956 68.17614087]]\n",
      "Agent 19) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[63.76794938 63.60464957]\n",
      " [65.86842772 65.91066032]]\n",
      "[[64.47007112 63.83220136]\n",
      " [64.63606003 64.66501134]]\n",
      "Agent 0) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Cooperate\n",
      "[[ 80.45258439  80.19404198]\n",
      " [ 83.50882681 148.71848288]]\n",
      "[[ 80.68929513  81.42187318]\n",
      " [ 87.13956403 150.22615587]]\n",
      "Agent 1) PS-Strategy: Always-Stay, PD-Strategy: Always-Cooperate\n",
      "[[ 86.43747732  86.44534947]\n",
      " [ 89.40928691 149.08814292]]\n",
      "[[ 87.04827788  87.41575987]\n",
      " [ 91.700093   150.59572114]]\n",
      "Agent 2) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 86.85911183  86.85490972]\n",
      " [ 89.54850343 149.14334933]]\n",
      "[[ 87.46230473  87.07224567]\n",
      " [ 91.85529831 150.65091339]]\n",
      "Agent 3) PS-Strategy: Always-Stay, PD-Strategy: Always-Cooperate\n",
      "[[ 81.22079524  81.29681868]\n",
      " [ 84.37398508 148.83287104]]\n",
      "[[ 81.76633253  81.77214589]\n",
      " [ 88.66614997 150.34051471]]\n",
      "Agent 4) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 89.34753466  89.41965009]\n",
      " [ 91.03757639 149.14514421]]\n",
      "[[ 90.55149455  88.85069882]\n",
      " [ 92.35202751 150.65270782]]\n",
      "Agent 5) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Cooperate\n",
      "[[ 78.41987536  77.79875857]\n",
      " [ 83.10625946 148.71018304]]\n",
      "[[ 78.28006489  79.53380774]\n",
      " [ 86.16382952 150.21785816]]\n",
      "Agent 6) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 85.20590243  85.25212732]\n",
      " [ 87.51424946 149.0150636 ]]\n",
      "[[ 85.87415191  85.66234794]\n",
      " [ 89.86684841 150.52266055]]\n",
      "Agent 7) PS-Strategy: Always-Stay, PD-Strategy: Always-Cooperate\n",
      "[[ 84.22833865  84.25866654]\n",
      " [ 88.19934757 149.03226121]]\n",
      "[[ 84.74182819  85.19982955]\n",
      " [ 90.73358883 150.53985376]]\n",
      "Agent 8) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 85.30266523  85.24329981]\n",
      " [ 88.08952898 149.01795667]]\n",
      "[[ 85.80365017  85.78666748]\n",
      " [ 90.78602474 150.52555288]]\n",
      "Agent 9) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 83.20048361  83.14458927]\n",
      " [ 85.99544898 149.02576199]]\n",
      "[[ 84.55611578  83.31417844]\n",
      " [ 89.30143225 150.5333562 ]]\n",
      "Agent 10) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[55.7526992  56.11095901]\n",
      " [77.47194771 80.94072137]]\n",
      "[[56.61189279 55.95070567]\n",
      " [85.05139856 83.27570987]]\n",
      "Agent 11) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 86.8411595   86.93251998]\n",
      " [ 89.05131043 149.08597516]]\n",
      "[[ 88.1409087   87.11660188]\n",
      " [ 90.41517128 150.59355394]]\n",
      "Agent 12) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 85.30360509  85.3006834 ]\n",
      " [ 88.17144188 149.03496972]]\n",
      "[[ 86.29210926  85.18917744]\n",
      " [ 89.98621434 150.54256157]]\n",
      "Agent 13) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 79.58578263  79.56729853]\n",
      " [ 83.83569349 148.82340327]]\n",
      "[[ 80.55204135  79.86216176]\n",
      " [ 87.10599383 150.33104936]]\n",
      "Agent 14) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[68.72034301 68.84052158]\n",
      " [76.8616763  79.13540584]]\n",
      "[[69.23215283 69.16407716]\n",
      " [82.09862742 80.0918149 ]]\n",
      "Agent 15) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 82.71344764  82.71028574]\n",
      " [ 86.00160208 149.02858672]]\n",
      "[[ 83.31620465  83.30155975]\n",
      " [ 87.42924073 150.5361802 ]]\n",
      "Agent 16) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 67.66549091  67.8708101 ]\n",
      " [ 78.6652169  143.98347844]]\n",
      "[[ 71.33941503  68.82084998]\n",
      " [ 81.80162398 145.49236538]]\n",
      "Agent 17) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 85.10825796  85.72317931]\n",
      " [ 86.98125202 149.04511299]]\n",
      "[[ 87.36599024  85.02384153]\n",
      " [ 89.46830319 150.55270224]]\n",
      "Agent 18) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Cooperate\n",
      "[[ 66.38109118  66.36028007]\n",
      " [ 75.3557027  144.06733997]]\n",
      "[[ 66.86462539  67.47501624]\n",
      " [ 81.83066152 145.57620541]]\n",
      "Agent 19) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Cooperate\n",
      "[[ 85.3841101   85.38138368]\n",
      " [ 88.36451502 149.04755572]]\n",
      "[[ 85.86123283  86.03688824]\n",
      " [ 90.41029362 150.55514435]]\n"
     ]
    }
   ],
   "source": [
    "base_params = {\"population\": 20,   # Agent Population Size (Must be a multiple of 2)\n",
    "     \"rounds\": 20,            # Rounds per Episode\n",
    "     \"episodes\": 2000,        # Number of Episodes\n",
    "     \"learning_rate\": 0.05,   # Alpha (Learning Rate)\n",
    "     \"temperature\": 85,       # Starting Boltzmann Temperature \n",
    "     \"discount_rate\": 0.99,   # Gamma (Discount Rate)\n",
    "     \"delta_t\": 0.99,         # Boltzmann Temperature Decay Rate\n",
    "     \"disposition\": 0.5,      # Disposition to Assume Cooperation\n",
    "     \"know_fresh_agent\": 0.0, # Probability of Knowing Fresh Agent's Previous Action\n",
    "     \"prefer_same_pool\": 0.0, # Probability of Choosing Same Pool Partner\n",
    "     \"prefer_different_pool\": 0.0, # Probability of Choosing Different Pool Partner\n",
    "     \"learning_mode\": \"q_learning\", # Learning Mode (q_learning or sarsa)\n",
    "     \"do_plot\": False}         # Plot Results\n",
    "\n",
    "tests = {\n",
    "    \"disposition\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"know_fresh_agent\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"prefer_same_pool\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"prefer_different_pool\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "params = base_params.copy()\n",
    "all_results = {}\n",
    "learning_type = \"q_learning\"\n",
    "\n",
    "for param_name, param_values in tests.items():\n",
    "    param_results = {}\n",
    "    for param_value in param_values:\n",
    "        params = base_params.copy()\n",
    "        params[param_name] = param_value\n",
    "        output = sdoo(**params)\n",
    "        param_results[param_value] = {\"params\": params.copy(), \"output\": output}\n",
    "    all_results[param_name] = param_results\n",
    "\n",
    "with open('{learning_type}_tests.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopulation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20\u001b[39m,   \u001b[38;5;66;03m# Agent Population Size (Must be a multiple of 2)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrounds\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20\u001b[39m,            \u001b[38;5;66;03m# Rounds per Episode\u001b[39;00m\n\u001b[0;32m      3\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1500\u001b[39m,        \u001b[38;5;66;03m# Number of Episodes\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_learning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# Learning Mode (q_learning or sarsa)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_plot\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}         \u001b[38;5;66;03m# Plot Results\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msdoo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 238\u001b[0m, in \u001b[0;36msdoo\u001b[1;34m(population, rounds, episodes, learning_rate, temperature, discount_rate, delta_t, disposition, know_fresh_agent, prefer_same_pool, prefer_different_pool, learning_mode, do_plot)\u001b[0m\n\u001b[0;32m    235\u001b[0m percentage_of_states_per_episode[(s_i\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m][episode] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    236\u001b[0m percentage_of_states_per_episode[(s_j\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m][episode] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 238\u001b[0m a_i \u001b[38;5;241m=\u001b[39m \u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_pd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m a_j \u001b[38;5;241m=\u001b[39m agents[j]\u001b[38;5;241m.\u001b[39mget_action_pd(s_j)\n\u001b[0;32m    241\u001b[0m tempi_pd \u001b[38;5;241m=\u001b[39m boltzmann_exploration(agents[i]\u001b[38;5;241m.\u001b[39mqtable_pd, s_i, agents[i]\u001b[38;5;241m.\u001b[39mt, \u001b[38;5;241m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 35\u001b[0m, in \u001b[0;36mAgent.get_action_pd\u001b[1;34m(self, state, debug)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action_pd\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: State, debug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ActionPD:\n\u001b[0;32m     34\u001b[0m     temp \u001b[38;5;241m=\u001b[39m boltzmann_exploration(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqtable_pd, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt, \u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mActionPD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFECT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mActionPD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOOPERATE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction Probabilities: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\"population\": 20,   # Agent Population Size (Must be a multiple of 2)\n",
    "     \"rounds\": 20,            # Rounds per Episode\n",
    "     \"episodes\": 1500,        # Number of Episodes\n",
    "     \"learning_rate\": 0.05,   # Alpha (Learning Rate)\n",
    "     \"temperature\": 85,       # Starting Boltzmann Temperature \n",
    "     \"discount_rate\": 0.99,   # Gamma (Discount Rate)\n",
    "     \"delta_t\": 0.992,        # Boltzmann Temperature Decay Rate\n",
    "     \"disposition\": 0.5,      # Disposition to Assume Cooperation\n",
    "     \"know_fresh_agent\": 0.0, # Probability of Knowing Fresh Agent's Previous Action\n",
    "     \"prefer_same_pool\": 0.0, # Probability of Choosing Same Pool Partner\n",
    "     \"prefer_different_pool\": 0.0, # Probability of Choosing Different Pool Partner\n",
    "     \"learning_mode\": \"q_learning\", # Learning Mode (q_learning or sarsa)\n",
    "     \"do_plot\": True}         # Plot Results\n",
    "\n",
    "output = sdoo(**params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evolution12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
