{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Tables\n",
    "\n",
    "| Partner Selection  | LEAVE | STAY |\n",
    "| :----------------- | :---: | :--: |\n",
    "| PARTNER_DEFECTED   | 0     | 0    |\n",
    "| PARTNER_COOPERATED | 0     | 0    |\n",
    "\n",
    "`LEAVE` and `STAY` are the two actions in the partner selection stage.  \n",
    "If two agents are paired and one agent chooses to leave then both agents will be added to a pool. \n",
    "After all agents have made their choices agents in the pool are randomly paired. \n",
    "If both agents choose to stay then they will remain paired for the prisoner's dilemma stage.\n",
    "\n",
    "| Prisoner's Dilemma  | DEFECT | COOPERATE |\n",
    "| :------------------ | :----: | :-------: |\n",
    "| PARTNER_DEFECTED    | 0      | 0         |\n",
    "| PARTNER_COOPERATED  | 0      | 0         |\n",
    "\n",
    "`DEFECT` and `COOPERATE` are the two actions in the prisoner's dilemma stage.\n",
    "Agents are given rewards in the prisoner's dilemma stage which can be seen in the table below \n",
    "where ($r_i$, $r_j$) is the returned rewards for agents `i` and `j`\n",
    "\n",
    "|                    | `j` Defects | `j` Cooperates |\n",
    "| ------------------ | ----------- | -------------- |\n",
    "| **`i` Defects**    | (1, 1)      | (5, 0)         |\n",
    "| **`i` Cooperates** | (0, 5)      | (3, 3)         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ActionPS(Enum):\n",
    "    LEAVE = 0\n",
    "    STAY = 1\n",
    "\n",
    "class ActionPD(Enum):\n",
    "    DEFECT = 0\n",
    "    COOPERATE = 1\n",
    "\n",
    "class State(Enum):\n",
    "    PARTNER_DEFECTED = 0\n",
    "    PARTNER_COOPERATED = 1\n",
    "\n",
    "def get_state(action: ActionPD) -> State:\n",
    "    return State.PARTNER_COOPERATED if action == ActionPD.COOPERATE else State.PARTNER_DEFECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyPS(Enum):\n",
    "    ALWAYS_STAY = 0\n",
    "    OUT_FOR_TAT = 1\n",
    "    REVERSE_OUT_FOR_TAT = 2\n",
    "    ALWAYS_LEAVE = 3\n",
    "    RANDOM = 4\n",
    "\n",
    "class StrategyPD(Enum):\n",
    "    ALWAYS_COOPERATE = 0\n",
    "    TIT_FOR_TAT = 1\n",
    "    REVERSE_TIT_FOR_TAT = 2\n",
    "    ALWAYS_DEFECT = 3\n",
    "    RANDOM = 4\n",
    "    \n",
    "strategy_names = {\n",
    "    StrategyPS.ALWAYS_STAY : 'Always-Stay',\n",
    "    StrategyPS.OUT_FOR_TAT : 'Out-For-Tat',\n",
    "    StrategyPS.REVERSE_OUT_FOR_TAT : 'Reverse-OFT',\n",
    "    StrategyPS.ALWAYS_LEAVE : 'Always-Leave',\n",
    "    StrategyPS.RANDOM : 'Random (PS)',\n",
    "    StrategyPD.ALWAYS_COOPERATE : 'Always-Cooperate',\n",
    "    StrategyPD.TIT_FOR_TAT : 'Tit-For-Tat',\n",
    "    StrategyPD.REVERSE_TIT_FOR_TAT : 'Reverse-TFT',\n",
    "    StrategyPD.ALWAYS_DEFECT : 'Always-Defect',\n",
    "    StrategyPD.RANDOM : 'Random (PD)',\n",
    "}\n",
    "\n",
    "strategy_colors = {\n",
    "    StrategyPS.ALWAYS_STAY : 'lightcoral',\n",
    "    StrategyPS.OUT_FOR_TAT : 'lightsteelblue',\n",
    "    StrategyPS.REVERSE_OUT_FOR_TAT : 'lightgreen',\n",
    "    StrategyPS.ALWAYS_LEAVE : 'tan',\n",
    "    StrategyPS.RANDOM : 'mediumpurple',\n",
    "    StrategyPD.ALWAYS_COOPERATE : 'red',\n",
    "    StrategyPD.TIT_FOR_TAT : 'blue',\n",
    "    StrategyPD.REVERSE_TIT_FOR_TAT : 'green',\n",
    "    StrategyPD.ALWAYS_DEFECT : 'yellow',\n",
    "    StrategyPD.RANDOM : 'purple',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the probabilities of selection actions given the current state \n",
    "def boltzmann_exploration(q_table, state: State, temperature: float, constant):\n",
    "    exp = np.exp((q_table[state.value, :] - max(q_table[state.value, :])) / temperature)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "def epsilon_greedy(q_table, state: State, epsilon: float) -> np.ndarray:\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.ones(len(q_table[state.value, :])) / len(q_table[state.value, :])\n",
    "    else:\n",
    "        prob = np.zeros(len(q_table[state.value, :]))\n",
    "        prob[np.argmax(q_table[state.value, :])] = 1\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs the Q-Learning algorithm on the provided qtable\n",
    "# NOTE: alpha is the learning rate and gamma is the discount rate\n",
    "def q_learning(qtable, next_qtable, state: State, action, \n",
    "               reward: float, new_state: State, alpha: float, gamma: float) -> None:\n",
    "    qtable[state.value, action.value] = (1.0 - alpha) * qtable[state.value, action.value] + \\\n",
    "        alpha * (reward + gamma * np.max(next_qtable[new_state.value, :]))\n",
    "    \n",
    "def sarsa_learning(qtable, next_qtable, state: State, action, \n",
    "               reward: float, new_state: State, new_action, alpha: float, gamma: float) -> None:\n",
    "    qtable[state.value, action.value] = (1.0 - alpha) * qtable[state.value, action.value] + \\\n",
    "        alpha * (reward + gamma * next_qtable[new_state.value, new_action.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, learning_rate: float, temperature: float, discount_rate: float, \n",
    "                 delta_t: float,\n",
    "                 last_action_pd: ActionPD = None, qtable_ps = None, qtable_pd = None):\n",
    "        self.a = learning_rate\n",
    "        self.t = temperature\n",
    "        self.g = discount_rate\n",
    "        self.delta_t = delta_t\n",
    "        self.last_action_pd = np.random.choice([ActionPD.DEFECT, ActionPD.COOPERATE], 1) if last_action_pd == None else last_action_pd\n",
    "        self.qtable_ps = np.zeros((2, 2)) if qtable_ps is None else qtable_ps\n",
    "        self.qtable_pd = np.zeros((2, 2)) if qtable_pd is None else qtable_pd\n",
    "        \n",
    "        # Force agents to use Out For Tat Partner Selection Strategy\n",
    "        # self.qtable_ps[0, 0] = 10\n",
    "        # self.qtable_ps[1, 1] = 10\n",
    "        \n",
    "    # returns an action given the current state\n",
    "    def get_action_ps(self, state: State, debug = False) -> ActionPS:\n",
    "        temp = boltzmann_exploration(self.qtable_ps, state, self.t, 0.8)\n",
    "        action = np.random.choice([ActionPS.LEAVE, ActionPS.STAY], p=temp)\n",
    "        if debug:\n",
    "            print(\"Action Probabilities: \" + str(temp))\n",
    "            print(\"Chosen Action: \" + str(action))\n",
    "        return action\n",
    "\n",
    "        # Force Out for Tat\n",
    "        # if state == State.PARTNER_COOPERATED:\n",
    "        #     return ActionPS.STAY\n",
    "        # else:\n",
    "        #     return ActionPS.LEAVE\n",
    "\n",
    "    def get_action_pd(self, state: State, debug = False) -> ActionPD:\n",
    "        temp = boltzmann_exploration(self.qtable_pd, state, self.t, 0.8)\n",
    "        action = np.random.choice([ActionPD.DEFECT, ActionPD.COOPERATE], p=temp)\n",
    "        if debug:\n",
    "            print(\"Action Probabilities: \" + str(temp))\n",
    "            print(\"Chosen Action: \" + str(action))\n",
    "        return action\n",
    "    \n",
    "    def update_reward(self, reward):\n",
    "        pass\n",
    "    \n",
    "    # trains using trajectories from each round\n",
    "    def train(self, trajectories, learning_mode, debug = False):\n",
    "        \n",
    "        # Author's Implementation\n",
    "\n",
    "        # discounted_rewards = []\n",
    "        # running_sum = 0\n",
    "        # for trajectory in trajectories[::-1]:\n",
    "        #     running_sum = trajectory[6] + self.g * running_sum\n",
    "        #     discounted_rewards.append(running_sum)\n",
    "        # discounted_rewards.reverse()\n",
    "\n",
    "        # for idx, trajectory in enumerate(trajectories):\n",
    "        #     # partner selection training\n",
    "        #     q_learning(self.qtable_ps, self.qtable_pd, trajectory[0], trajectory[1], discounted_rewards[idx], trajectory[3], self.a, self.g)\n",
    "        #     # prisoner's dilemma training\n",
    "        #     q_learning(self.qtable_pd, self.qtable_ps, trajectory[3], trajectory[4], discounted_rewards[idx], trajectory[5], self.a, self.g)\n",
    "\n",
    "        # Interpreted Implementation\n",
    "\n",
    "        if learning_mode == \"q_learning\":\n",
    "            for idx, trajectory in enumerate(trajectories):\n",
    "                # partner selection training\n",
    "                q_learning(self.qtable_ps, self.qtable_pd, trajectory[0], trajectory[1], trajectory[2], trajectory[3], self.a, self.g)\n",
    "                # prisoner's dilemma training\n",
    "                q_learning(self.qtable_pd, self.qtable_ps, trajectory[3], trajectory[4], trajectory[6], trajectory[5], self.a, self.g)\n",
    "        elif learning_mode == \"sarsa\":\n",
    "            for idx, trajectory in enumerate(trajectories):\n",
    "                # partner selection training\n",
    "                sarsa_learning(self.qtable_ps, self.qtable_pd, trajectory[0], trajectory[1], trajectory[2], trajectory[3], trajectory[4], self.a, self.g)\n",
    "                if idx > 0:\n",
    "                    # prisoner's dilemma training\n",
    "                    sarsa_learning(self.qtable_pd, self.qtable_ps, last_trajectory[3], last_trajectory[4], last_trajectory[6], last_trajectory[5], trajectory[1], self.a, self.g)\n",
    "                last_trajectory = trajectory\n",
    "                \n",
    "        # decrease temperature\n",
    "        # self.t *= 0.01\n",
    "        self.t *= self.delta_t\n",
    "    \n",
    "    def get_strategy_ps(self):\n",
    "        if (self.qtable_ps[0, 0] < self.qtable_ps[0, 1] and self.qtable_ps[1, 0] < self.qtable_ps[1, 1]):\n",
    "            return StrategyPS.ALWAYS_STAY\n",
    "        elif (self.qtable_ps[0, 0] > self.qtable_ps[0, 1] and self.qtable_ps[1, 0] < self.qtable_ps[1, 1]):\n",
    "            return StrategyPS.OUT_FOR_TAT\n",
    "        elif (self.qtable_ps[0, 0] < self.qtable_ps[0, 1] and self.qtable_ps[1, 0] > self.qtable_ps[1, 1]):\n",
    "            return StrategyPS.REVERSE_OUT_FOR_TAT\n",
    "        elif (self.qtable_ps[0, 0] > self.qtable_ps[0, 1] and self.qtable_ps[1, 0] > self.qtable_ps[1, 1]):\n",
    "            return StrategyPS.ALWAYS_LEAVE\n",
    "        else:\n",
    "            return StrategyPS.RANDOM\n",
    "\n",
    "    def get_strategy_pd(self):\n",
    "        if (self.qtable_pd[0, 0] < self.qtable_pd[0, 1] and self.qtable_pd[1, 0] < self.qtable_pd[1, 1]):\n",
    "            return StrategyPD.ALWAYS_COOPERATE\n",
    "        elif (self.qtable_pd[0, 0] > self.qtable_pd[0, 1] and self.qtable_pd[1, 0] < self.qtable_pd[1, 1]):\n",
    "            return StrategyPD.TIT_FOR_TAT\n",
    "        elif (self.qtable_pd[0, 0] < self.qtable_pd[0, 1] and self.qtable_pd[1, 0] > self.qtable_pd[1, 1]):\n",
    "            return StrategyPD.REVERSE_TIT_FOR_TAT\n",
    "        elif (self.qtable_pd[0, 0] > self.qtable_pd[0, 1] and self.qtable_pd[1, 0] > self.qtable_pd[1, 1]):\n",
    "            return StrategyPD.ALWAYS_DEFECT\n",
    "        else:\n",
    "            return StrategyPD.RANDOM\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the rewards of two agents in the prisoner's dilemma game\n",
    "def prisoners_dilemma(a_i: ActionPD, a_j: ActionPD) -> tuple[float, float]:\n",
    "    reward_table = np.array([[(1, 1), (5, 0)], [(0, 5), (3, 3)]])\n",
    "    return reward_table[a_i.value, a_j.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectories\n",
    "Each agent tracks the folowing figures each round as a trajectory:  \n",
    "- $(s_{PS}, a_{PS}, r_{PS} = 0, s'_{PS} = s_{PD}, a_{PD}, s'_{PD}, r_{PD})$\n",
    "- $s_{PS}$ is the state of the agent in the partner selection stage.  \n",
    "- $a_{PS}$ is the action the agent made in the partner selection stage.  \n",
    "- $r_{PS}$ is the reward the agent recieved from the partner selection stage. ~~This will always be zero and is not recorded~~.  \n",
    "- $s'_{PS}$ is the new state after the prisoner selection stage which is equivalent to the state in the prisoner's dilemma stage $s_{PD}$.  \n",
    "- $a_{PD}$ is the action the agent made in the prisoner's dilemma stage.  \n",
    "- $s'_{PS}$ is the new state after the prisoner's dilemma stage.  \n",
    "- $r_{PD}$ is the reward the agent recieved from the prisoners dilemma stage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdoo(population: int, rounds: int, episodes: int, learning_rate: float, temperature: float, discount_rate: float, delta_t: float,\n",
    "         disposition: float, know_fresh_agent: float, prefer_same_pool: float, prefer_different_pool: float, learning_mode: str = \"q_learning\",\n",
    "         do_plot: bool = True):\n",
    "    if (population % 2 != 0):\n",
    "        print(\"sdoo: population must be a multiple of two\")\n",
    "        return\n",
    "    if prefer_same_pool + prefer_different_pool > 1.0:\n",
    "        print(\"sdoo: prefer_same_pool + prefer_different_pool must be less than or equal to 1.0\")\n",
    "        return\n",
    "    \n",
    "    recorded_outcomes_pd = {\n",
    "        (ActionPD.DEFECT, ActionPD.DEFECT): [0 for _ in range(episodes)],\n",
    "        (ActionPD.DEFECT, ActionPD.COOPERATE): [0 for _ in range(episodes)],\n",
    "        (ActionPD.COOPERATE, ActionPD.DEFECT): [0 for _ in range(episodes)],\n",
    "        (ActionPD.COOPERATE, ActionPD.COOPERATE): [0 for _ in range(episodes)],\n",
    "    }\n",
    "\n",
    "    recorded_agent_strategy_pairings = {\n",
    "        # (a, b): [0 for _ in range(episodes)] for a in StrategyPD for b in StrategyPD\n",
    "        (StrategyPD.ALWAYS_COOPERATE, StrategyPD.ALWAYS_COOPERATE): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.ALWAYS_COOPERATE, StrategyPD.TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.ALWAYS_COOPERATE, StrategyPD.REVERSE_TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.ALWAYS_COOPERATE, StrategyPD.ALWAYS_DEFECT): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.ALWAYS_COOPERATE, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.TIT_FOR_TAT, StrategyPD.TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.TIT_FOR_TAT, StrategyPD.REVERSE_TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.TIT_FOR_TAT, StrategyPD.ALWAYS_DEFECT): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.TIT_FOR_TAT, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.REVERSE_TIT_FOR_TAT, StrategyPD.REVERSE_TIT_FOR_TAT): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.REVERSE_TIT_FOR_TAT, StrategyPD.ALWAYS_DEFECT): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.REVERSE_TIT_FOR_TAT, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "        (StrategyPD.ALWAYS_DEFECT, StrategyPD.ALWAYS_DEFECT): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.ALWAYS_DEFECT, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "        # (StrategyPD.RANDOM, StrategyPD.RANDOM): [0 for _ in range(episodes)],\n",
    "    }\n",
    "\n",
    "    recorded_outcome_changes = {\n",
    "        ((ActionPD.COOPERATE, ActionPD.COOPERATE), (ActionPD.COOPERATE, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.COOPERATE), (ActionPD.COOPERATE, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.COOPERATE), (ActionPD.DEFECT, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.COOPERATE), (ActionPD.DEFECT, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "\n",
    "        ((ActionPD.COOPERATE, ActionPD.DEFECT), (ActionPD.COOPERATE, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.DEFECT), (ActionPD.COOPERATE, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.DEFECT), (ActionPD.DEFECT, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.COOPERATE, ActionPD.DEFECT), (ActionPD.DEFECT, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "\n",
    "        ((ActionPD.DEFECT, ActionPD.COOPERATE), (ActionPD.COOPERATE, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.COOPERATE), (ActionPD.COOPERATE, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.COOPERATE), (ActionPD.DEFECT, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.COOPERATE), (ActionPD.DEFECT, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "\n",
    "        ((ActionPD.DEFECT, ActionPD.DEFECT), (ActionPD.COOPERATE, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.DEFECT), (ActionPD.COOPERATE, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.DEFECT), (ActionPD.DEFECT, ActionPD.COOPERATE)): [0 for _ in range(episodes)],\n",
    "        ((ActionPD.DEFECT, ActionPD.DEFECT), (ActionPD.DEFECT, ActionPD.DEFECT)): [0 for _ in range(episodes)],\n",
    "    }\n",
    "\n",
    "    agent_qvales_ps = [[0 for _ in range(episodes)] for _ in range(4)]\n",
    "    agent_qvales_pd = [[0 for _ in range(episodes)] for _ in range(4)]\n",
    "\n",
    "    recorded_qvalues_ps = [[[0 for _ in range(episodes)] for _ in range(4)] for _ in range(population)]\n",
    "    recorded_qvalues_pd = [[[0 for _ in range(episodes)] for _ in range(4)] for _ in range(population)]\n",
    "\n",
    "    agent_ps_actions_per_episode = [[0 for _ in range(episodes)] for _ in range(4)]\n",
    "    percentage_of_states_per_episode = [[0 for _ in range(episodes)] for _ in range(4)]\n",
    "\n",
    "    agent_chosen_switches_per_episode = [0 for _ in range(episodes)]\n",
    "    agent_switches_per_episode = [0 for _ in range(episodes)]\n",
    "\n",
    "    total_reward = [0 for _ in range(episodes)]\n",
    "\n",
    "    # Fix Randoms\n",
    "    # np.random.seed(0)\n",
    "\n",
    "    # Global Q-Table Test\n",
    "    # qtable_ps = np.zeros((2, 2))\n",
    "    # qtable_pd = np.zeros((2, 2))\n",
    "    # agents = [Agent(learning_rate, temperature, discount_rate, qtable_ps=qtable_ps, qtable_pd=qtable_pd) for _ in range(population)]\n",
    "\n",
    "    agents = [Agent(learning_rate, temperature, discount_rate, delta_t=delta_t) for _ in range(population)]\n",
    "    unpaired = list(range(population))\n",
    "\n",
    "# Pair Agents\n",
    "    pairs: tuple[int, int] = []\n",
    "    while unpaired:\n",
    "        i = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "        j = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "        pairs.append((i, j))\n",
    "\n",
    "    probabilities_ps_defected = []\n",
    "    probabilities_ps_cooperated = []\n",
    "    probabilities_pd_defected = []\n",
    "    probabilities_pd_cooperated = []\n",
    "    new_probabilities_ps_defected = []\n",
    "    new_probabilities_pd_defected = []\n",
    "    new_probabilities_ps_cooperated = []\n",
    "    new_probabilities_pd_cooperated = []\n",
    "    strategies_ps = []\n",
    "    strategies_pd = []\n",
    "    new_strategies_ps = []\n",
    "    new_strategies_pd = []\n",
    "    \n",
    "    # probabilities_ps_i.append()\n",
    "    # probabilities_ps_j.append(0.1)\n",
    "    # probabilities_pd_i.append(0.1)\n",
    "    # probabilities_pd_j.append(0.1)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Record agent Q-Values\n",
    "        for agent_idx in range(len(recorded_qvalues_ps)):\n",
    "            for idx in range(len(recorded_qvalues_ps[agent_idx])):\n",
    "                recorded_qvalues_ps[agent_idx][idx][episode] = agents[agent_idx].qtable_ps.ravel()[idx]\n",
    "\n",
    "        for agent_idx in range(len(recorded_qvalues_pd)):\n",
    "            for idx in range(len(recorded_qvalues_pd[agent_idx])):\n",
    "                recorded_qvalues_pd[agent_idx][idx][episode] = agents[agent_idx].qtable_pd.ravel()[idx]\n",
    "\n",
    "        trajectories = [[] for _ in range(population)]\n",
    "        for round in range(rounds):\n",
    "\n",
    "            # Partner Selection\n",
    "            temp_pairs = []\n",
    "            switch_leave_pool = []\n",
    "            switch_stay_pool = []\n",
    "            stay_pool = []\n",
    "\n",
    "            for (i, j) in pairs:\n",
    "                s_i = get_state(agents[j].last_action_pd)\n",
    "                s_j = get_state(agents[i].last_action_pd)\n",
    "                percentage_of_states_per_episode[s_i.value][episode] += 1\n",
    "                percentage_of_states_per_episode[s_j.value][episode] += 1\n",
    "\n",
    "                a_i = agents[i].get_action_ps(s_i)\n",
    "                a_j = agents[j].get_action_ps(s_j)\n",
    "\n",
    "                tempi_ps = boltzmann_exploration(agents[i].qtable_ps, s_i, agents[i].t, 0.8)\n",
    "                tempj_ps = boltzmann_exploration(agents[j].qtable_ps, s_j, agents[j].t, 0.8)\n",
    "\n",
    "                if s_i == State.PARTNER_DEFECTED:\n",
    "                    probabilities_ps_defected.append(tempi_ps[0])\n",
    "                else:\n",
    "                    probabilities_ps_cooperated.append(tempi_ps[0])\n",
    "                if s_j == State.PARTNER_DEFECTED:\n",
    "                    probabilities_ps_defected.append(tempj_ps[0])\n",
    "                else:\n",
    "                    probabilities_ps_cooperated.append(tempj_ps[0])\n",
    "\n",
    "                if a_i == ActionPS.LEAVE or a_j == ActionPS.LEAVE:\n",
    "                    if a_i == ActionPS.LEAVE:\n",
    "                        switch_leave_pool.append(i)\n",
    "                        agents[i].last_action_ps = ActionPS.LEAVE\n",
    "                        agents[i].last_result_ps = \"split\"\n",
    "                    else:\n",
    "                        switch_stay_pool.append(i)\n",
    "                        agents[i].last_action_ps = ActionPS.STAY\n",
    "                        agents[i].last_result_ps = \"split\"\n",
    "                    if a_j == ActionPS.LEAVE:\n",
    "                        switch_leave_pool.append(j)\n",
    "                        agents[j].last_action_ps = ActionPS.LEAVE\n",
    "                        agents[j].last_result_ps = \"split\"\n",
    "                    else:\n",
    "                        switch_stay_pool.append(j)\n",
    "                        agents[j].last_action_ps = ActionPS.STAY\n",
    "                        agents[j].last_result_ps = \"split\"\n",
    "                    agent_switches_per_episode[episode] += 2\n",
    "                    agent_chosen_switches_per_episode[episode] += int(a_i == ActionPS.LEAVE) + int(a_j == ActionPS.LEAVE)\n",
    "                else:\n",
    "                    stay_pool.append((i, j))\n",
    "                    agents[i].last_action_ps = ActionPS.STAY\n",
    "                    agents[i].last_result_ps = \"stay\"\n",
    "                    agents[j].last_action_ps = ActionPS.STAY\n",
    "                    agents[j].last_result_ps = \"stay\"\n",
    "\n",
    "                # Partner Selection Rewards Test\n",
    "                # r_i = 1 if a_j == ActionPS.STAY else -1\n",
    "                # r_j = 1 if a_i == ActionPS.STAY else -1\n",
    "                r_i = 0\n",
    "                r_j = 0\n",
    "\n",
    "                trajectories[i].append((s_i, a_i, r_i))\n",
    "                trajectories[j].append((s_j, a_j, r_j))\n",
    "\n",
    "            # Pair Agents in switch_pool and switched_pool\n",
    "            while switch_leave_pool and switch_stay_pool:\n",
    "                full_pool = switch_leave_pool + switch_stay_pool\n",
    "                i = full_pool.pop(np.random.randint(len(full_pool)))\n",
    "                i_pool = switch_leave_pool if i in switch_leave_pool else switch_stay_pool\n",
    "                i_pool.remove(i)\n",
    "                other_pool = switch_leave_pool if i_pool == switch_stay_pool else switch_stay_pool\n",
    "                rand_choice = np.random.rand()\n",
    "                if rand_choice < prefer_same_pool and i_pool:\n",
    "                    j_pool = i_pool\n",
    "                    j = j_pool.pop(np.random.randint(len(j_pool)))\n",
    "                elif rand_choice < prefer_same_pool + prefer_different_pool and other_pool:\n",
    "                    j_pool = other_pool\n",
    "                    j = j_pool.pop(np.random.randint(len(j_pool)))\n",
    "                else:\n",
    "                    j = full_pool.pop(np.random.randint(len(full_pool)))\n",
    "                    j_pool = switch_leave_pool if j in switch_leave_pool else switch_stay_pool\n",
    "                    j_pool.remove(j)\n",
    "                temp_pairs.append((i, j))\n",
    "\n",
    "            # Pair remaining agents in switch_pool with each other\n",
    "            while len(switch_leave_pool) > 1:\n",
    "                i = switch_leave_pool.pop(np.random.randint(len(switch_leave_pool)))\n",
    "                j = switch_leave_pool.pop(np.random.randint(len(switch_leave_pool)))\n",
    "                temp_pairs.append((i, j))\n",
    "\n",
    "            # Pair remaining agents in switched_pool with each other\n",
    "            while len(switch_stay_pool) > 1:\n",
    "                i = switch_stay_pool.pop(np.random.randint(len(switch_stay_pool)))\n",
    "                j = switch_stay_pool.pop(np.random.randint(len(switch_stay_pool)))\n",
    "                temp_pairs.append((i, j))\n",
    "\n",
    "            # Combine new pairs with stay_pool pairs\n",
    "            pairs = temp_pairs + stay_pool\n",
    "\n",
    "            # Prisoner's Dilemma\n",
    "            for (i, j) in pairs:\n",
    "                strategy_i = agents[i].get_strategy_pd()\n",
    "                strategy_j = agents[j].get_strategy_pd()\n",
    "                if (strategy_i, strategy_j) in recorded_agent_strategy_pairings:\n",
    "                    recorded_agent_strategy_pairings[(strategy_i, strategy_j)][episode] += 1\n",
    "                elif (strategy_j, strategy_i) in recorded_agent_strategy_pairings:\n",
    "                    recorded_agent_strategy_pairings[(strategy_j, strategy_i)][episode] += 1\n",
    "\n",
    "                # If agent split, use random action\n",
    "                if (agents[i].last_result_ps == \"split\" and agents[j].last_result_ps == \"split\") and np.random.rand() > know_fresh_agent:\n",
    "                    s_i = State.PARTNER_COOPERATED if np.random.rand() < disposition else State.PARTNER_DEFECTED\n",
    "                    s_j = State.PARTNER_COOPERATED if np.random.rand() < disposition else State.PARTNER_DEFECTED\n",
    "                else:\n",
    "                    s_i = get_state(agents[j].last_action_pd)\n",
    "                    s_j = get_state(agents[i].last_action_pd)\n",
    "                percentage_of_states_per_episode[(s_i.value) + 2][episode] += 1\n",
    "                percentage_of_states_per_episode[(s_j.value) + 2][episode] += 1\n",
    "\n",
    "                a_i = agents[i].get_action_pd(s_i)\n",
    "                a_j = agents[j].get_action_pd(s_j)\n",
    "\n",
    "                tempi_pd = boltzmann_exploration(agents[i].qtable_pd, s_i, agents[i].t, 0.8)\n",
    "                tempj_pd = boltzmann_exploration(agents[j].qtable_pd, s_j, agents[j].t, 0.8)\n",
    "\n",
    "                if s_i == State.PARTNER_DEFECTED:\n",
    "                    probabilities_pd_defected.append(tempi_pd[0])\n",
    "                else:\n",
    "                    probabilities_pd_cooperated.append(tempi_pd[0])\n",
    "                if s_j == State.PARTNER_DEFECTED:\n",
    "                    probabilities_pd_defected.append(tempj_pd[0])\n",
    "                else:\n",
    "                    probabilities_pd_cooperated.append(tempj_pd[0])\n",
    "\n",
    "                r_i, r_j = prisoners_dilemma(a_i, a_j)\n",
    "                total_reward[episode] += r_i + r_j\n",
    "\n",
    "                ns_i = get_state(a_j)\n",
    "                ns_j = get_state(a_i)\n",
    "                recorded_outcomes_pd[(a_i, a_j)][episode] += 1\n",
    "                agents[i].last_action_pd = a_i\n",
    "                agents[j].last_action_pd = a_j\n",
    "\n",
    "                # Record Trajectories\n",
    "                t = trajectories[i][round]\n",
    "                trajectories[i][round] = (t[0], t[1], t[2], s_i, a_i, ns_i, r_i)\n",
    "                t = trajectories[j][round]\n",
    "                trajectories[j][round] = (t[0], t[1], t[2], s_j, a_j, ns_j, r_j)\n",
    "\n",
    "                # Record Actions taken\n",
    "                agent_ps_actions_per_episode[2 * (s_i.value - 2) + a_i.value][episode] += 1\n",
    "                agent_ps_actions_per_episode[2 * (s_j.value - 2) + a_j.value][episode] += 1\n",
    "\n",
    "        # print()\n",
    "        \n",
    "        for idx, agent in enumerate(agents):\n",
    "            agent.train(trajectories[idx], learning_mode)\n",
    "            # agent.train(trajectories[idx], debug=(idx == 0))\n",
    "        \n",
    "        for idx in range(len(agent_qvales_ps)):\n",
    "            agent_qvales_ps[idx][episode] = np.sum([agent.qtable_ps.ravel()[idx] for agent in agents]) / len(agents)\n",
    "\n",
    "        for idx in range(len(agent_qvales_pd)):\n",
    "            agent_qvales_pd[idx][episode] = np.sum([agent.qtable_pd.ravel()[idx] for agent in agents]) / len(agents)\n",
    "\n",
    "        recorded_outcomes_pd[(ActionPD.DEFECT, ActionPD.DEFECT)][episode] /= (rounds * population / 2)\n",
    "        recorded_outcomes_pd[(ActionPD.DEFECT, ActionPD.COOPERATE)][episode] /= (rounds * population / 2)\n",
    "        recorded_outcomes_pd[(ActionPD.COOPERATE, ActionPD.DEFECT)][episode] /= (rounds * population / 2)\n",
    "        recorded_outcomes_pd[(ActionPD.COOPERATE, ActionPD.COOPERATE)][episode] /= (rounds * population / 2)\n",
    "\n",
    "        agent_ps_actions_per_episode[0][episode] /= (rounds * population)\n",
    "        agent_ps_actions_per_episode[1][episode] /= (rounds * population)\n",
    "        agent_ps_actions_per_episode[2][episode] /= (rounds * population)\n",
    "        agent_ps_actions_per_episode[3][episode] /= (rounds * population)\n",
    "\n",
    "        percentage_of_states_per_episode[0][episode] /= (rounds * population)\n",
    "        percentage_of_states_per_episode[1][episode] /= (rounds * population)\n",
    "        percentage_of_states_per_episode[2][episode] /= (rounds * population)\n",
    "        percentage_of_states_per_episode[3][episode] /= (rounds * population)\n",
    "\n",
    "        for idx, agent in enumerate(agents):\n",
    "            strategies_ps.append(agent.get_strategy_ps())\n",
    "            strategies_pd.append(agent.get_strategy_pd())\n",
    "\n",
    "        for agent_trajectories in trajectories:\n",
    "            for idx in range(rounds - 1):\n",
    "                outcome = (agent_trajectories[idx][4], ActionPD.COOPERATE if agent_trajectories[idx][5] == State.PARTNER_COOPERATED else ActionPD.DEFECT)\n",
    "                next_outcome = (agent_trajectories[idx + 1][4], ActionPD.COOPERATE if agent_trajectories[idx + 1][5] == State.PARTNER_COOPERATED else ActionPD.DEFECT)\n",
    "                # if (outcome, next_outcome) in recorded_outcome_changes:\n",
    "                recorded_outcome_changes[(outcome, next_outcome)][episode] += 1\n",
    "                # else:\n",
    "                #     recorded_outcome_changes[((outcome[1], outcome[0]), (next_outcome[1], next_outcome[0]))][episode] += 1\n",
    "        mean_probabilities_ps_defected = np.mean(probabilities_ps_defected)\n",
    "        mean_probabilities_ps_cooperated = np.mean(probabilities_ps_cooperated)\n",
    "        mean_probabilities_pd_defected = np.mean(probabilities_pd_defected)\n",
    "        mean_probabilities_pd_cooperated = np.mean(probabilities_pd_cooperated)\n",
    "        new_probabilities_ps_defected.append(mean_probabilities_ps_defected)\n",
    "        new_probabilities_ps_cooperated.append(mean_probabilities_ps_cooperated)\n",
    "        new_probabilities_pd_defected.append(mean_probabilities_pd_defected)\n",
    "        new_probabilities_pd_cooperated.append(mean_probabilities_pd_cooperated)\n",
    "        new_strategies_ps.append(strategies_ps)\n",
    "        new_strategies_pd.append(strategies_pd)\n",
    "        probabilities_ps_defected = []\n",
    "        probabilities_ps_cooperated = []\n",
    "        probabilities_pd_defected = []\n",
    "        probabilities_pd_cooperated = []\n",
    "        strategies_ps = []\n",
    "        strategies_pd = []\n",
    "\n",
    "    strat_always_leave = [sum([1 for strategy in strategies if strategy == StrategyPS.ALWAYS_LEAVE]) for strategies in new_strategies_ps]\n",
    "    strat_out_for_tat = [sum([1 for strategy in strategies if strategy == StrategyPS.OUT_FOR_TAT]) for strategies in new_strategies_ps]\n",
    "    strat_reverse_out_for_tat = [sum([1 for strategy in strategies if strategy == StrategyPS.REVERSE_OUT_FOR_TAT]) for strategies in new_strategies_ps]\n",
    "    strat_always_stay = [sum([1 for strategy in strategies if strategy == StrategyPS.ALWAYS_STAY]) for strategies in new_strategies_ps]\n",
    "    strat_always_defect = [sum([1 for strategy in strategies if strategy == StrategyPD.ALWAYS_DEFECT]) for strategies in new_strategies_pd]\n",
    "    strat_tit_for_tat = [sum([1 for strategy in strategies if strategy == StrategyPD.TIT_FOR_TAT]) for strategies in new_strategies_pd]\n",
    "    strat_reverse_tit_for_tat = [sum([1 for strategy in strategies if strategy == StrategyPD.REVERSE_TIT_FOR_TAT]) for strategies in new_strategies_pd]\n",
    "    strat_always_cooperate = [sum([1 for strategy in strategies if strategy == StrategyPD.ALWAYS_COOPERATE]) for strategies in new_strategies_pd]\n",
    "\n",
    "    # for agent_idx in range(recorded_qvalues_ps):\n",
    "    #         for idx in range(recorded_qvalues_ps[agent_idx]):\n",
    "    #             recorded_qvalues_ps[agent_idx][idx][episodes] = agent.qtable_ps.ravel()[idx]\n",
    "\n",
    "    if do_plot:\n",
    "        # Plot Prisoner's Dilemma Outcomes\n",
    "        plt.plot(recorded_outcomes_pd[(ActionPD.DEFECT, ActionPD.DEFECT)], linewidth=1)\n",
    "        plt.plot(recorded_outcomes_pd[(ActionPD.DEFECT, ActionPD.COOPERATE)], linewidth=1)\n",
    "        plt.plot(recorded_outcomes_pd[(ActionPD.COOPERATE, ActionPD.DEFECT)], linewidth=1)\n",
    "        plt.plot(recorded_outcomes_pd[(ActionPD.COOPERATE, ActionPD.COOPERATE)], linewidth=1)\n",
    "        \n",
    "        plt.title(\"Percentage of Prisoner's Dilemma Outcomes Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Percentage of Outcomes')\n",
    "        plt.legend([\"(D, D)\", \"(D, C)\", \"(C, D)\", \"(C, C)\", \"Total Reward\"])\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Strategies\n",
    "\n",
    "        plt.plot(strat_always_leave, linewidth=1)\n",
    "        plt.plot(strat_out_for_tat, linewidth=1)\n",
    "        plt.plot(strat_reverse_out_for_tat, linewidth=1)\n",
    "        plt.plot(strat_always_stay, linewidth=1)\n",
    "\n",
    "        plt.title(\"Number of Partner Selection Strategies Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Number of Agents')\n",
    "        plt.legend([\"Always Leave\", \"Out For Tat\", \"Reverse Out For Tat\", \"Always Stay\"])\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(strat_always_defect)\n",
    "        plt.plot(strat_tit_for_tat)\n",
    "        plt.plot(strat_reverse_tit_for_tat)\n",
    "        plt.plot(strat_always_cooperate)\n",
    "\n",
    "        plt.title(\"Number of Prisoner's Dilemma Strategies Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Number of Agents')\n",
    "        plt.legend([\"Always Defect\", \"Tit For Tat\", \"Reverse Tit For Tat\", \"Always Cooperate\"])\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Mean Probabilities of Defection and Cooperation\n",
    "        plt.title(\"Probabilities of defection given state in Prisoner's Dilemma Stage\")\n",
    "        plt.plot(new_probabilities_pd_defected)\n",
    "        plt.plot(new_probabilities_pd_cooperated)\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.legend([\"Partner Defected\", \"Partner Cooperated\"])\n",
    "        plt.ylabel(\"Probability of Defection\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.title(\"Probabilities of leaving given state in Prisoner's Dilemma Stage\")\n",
    "        plt.plot(new_probabilities_ps_defected)\n",
    "        plt.plot(new_probabilities_ps_cooperated)\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.legend([\"Partner Defected\", \"Partner Cooperated\"])\n",
    "        plt.ylabel(\"Probability of Leaving\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        # Plot Total Rewards\n",
    "        plt.plot(total_reward, linewidth=3)\n",
    "\n",
    "        plt.title(\"Total Reward Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.ylim(0, 6 * population * rounds / 2)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Plot Agent/Pair Switches Per Episode\n",
    "        plt.plot(np.divide(agent_chosen_switches_per_episode, rounds * population))\n",
    "        plt.plot(np.divide(agent_switches_per_episode, rounds * population))\n",
    "\n",
    "        plt.title(\"Percentage of Switches Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Percentage of Agents')\n",
    "        plt.legend([\"Agents Who Chose to Switch Partners\", \"Agents Who Switched Partners\"])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Plot Percentage of Agent PD Actions Per Episode Given State\n",
    "        plt.plot(agent_ps_actions_per_episode[0])\n",
    "        plt.plot(agent_ps_actions_per_episode[1])\n",
    "        plt.plot(agent_ps_actions_per_episode[2])\n",
    "        plt.plot(agent_ps_actions_per_episode[3])\n",
    "        \n",
    "        plt.title(\"Percentage of PD Actions Per Episode Given State\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Percentage of Agents')\n",
    "        plt.legend([\"Defected Given Parter Previously Defected\", \"Cooperated Given Parter Previously Defected\", \n",
    "                    \"Defected Given Parter Previously Cooperated\", \"Cooperated Given Parter Previously Cooperated\"],\n",
    "                    loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Percentage of Agent States Per Episode\n",
    "        plt.subplot(211)\n",
    "        plt.plot(percentage_of_states_per_episode[0])\n",
    "        plt.plot(percentage_of_states_per_episode[1])\n",
    "        \n",
    "        plt.title(\"Percentage of Partner Selection Agent States Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Percentage of States')\n",
    "        plt.legend([\"Partner Selection Where Parter Previously Defected\", \"Partner Selection Where Parter Previously Cooperated\"], \n",
    "                    loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.plot(percentage_of_states_per_episode[2], linestyle='dotted')\n",
    "        plt.plot(percentage_of_states_per_episode[3], linestyle='dotted')\n",
    "        \n",
    "        plt.title(\"Percentage of Prisoner's Dilemma Agent States Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Percentage of States')\n",
    "        plt.legend([\"Prisoner's Dilemma Where Parter Previously Defected\", \"Prisoner's Dilemma Where Parter Previously Cooperated\"], \n",
    "                    loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    num_strategies_ps = [0 for _ in StrategyPS]\n",
    "    num_strategies_pd = [0 for _ in StrategyPD]\n",
    "\n",
    "    strategy_combinations = np.zeros((len(StrategyPS), len(StrategyPD)))\n",
    "\n",
    "    # Determine Agent Strategies\n",
    "    for idx, agent in enumerate(agents):\n",
    "        strategy_ps = agent.get_strategy_ps()\n",
    "        strategy_pd = agent.get_strategy_pd()\n",
    "        \n",
    "        num_strategies_ps[strategy_ps.value] += 1\n",
    "        num_strategies_pd[strategy_pd.value] += 1\n",
    "        strategy_combinations[strategy_ps.value, strategy_pd.value] += 1\n",
    "        \n",
    "        print(\"Agent %i) PS-Strategy: %s, PD-Strategy: %s\" % \n",
    "            (idx, strategy_names[strategy_ps], strategy_names[strategy_pd]))\n",
    "        print(agent.qtable_ps)\n",
    "        print(agent.qtable_pd)\n",
    "\n",
    "    if do_plot:\n",
    "        plt.subplot(211)\n",
    "        ps_colors = [strategy_colors[strategy] for strategy in StrategyPS]\n",
    "        plt.bar([strategy_names[strategy] for strategy in StrategyPS], num_strategies_ps, color=ps_colors)\n",
    "        plt.title('PS-Strategies')\n",
    "        plt.xlabel('Strategy')\n",
    "        plt.ylabel('Number of Agents')\n",
    "        plt.ylim(0, population)\n",
    "        plt.show()\n",
    "\n",
    "        plt.subplot(212)\n",
    "        pd_colors = [strategy_colors[strategy] for strategy in StrategyPD]\n",
    "        plt.bar([strategy_names[strategy] for strategy in StrategyPD], num_strategies_pd, color=pd_colors)\n",
    "        plt.title('PD-Strategies')\n",
    "        plt.xlabel('Strategy')\n",
    "        plt.ylabel('Number of Agents')\n",
    "        plt.ylim(0, population)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Plot agent partner selection and prisoner's dilemma strategy combinations\n",
    "        combination_indices = [i for i in range(len(strategy_combinations.ravel()))]\n",
    "        ps_combination_colors = np.repeat([strategy_colors[strategy] for strategy in StrategyPS], 5)\n",
    "        pd_combination_colors = np.tile([strategy_colors[strategy] for strategy in StrategyPD], 5)\n",
    "        plt.bar(combination_indices, strategy_combinations.ravel() / 2.0, color=ps_combination_colors)\n",
    "        plt.bar(combination_indices, strategy_combinations.ravel() / 2.0, bottom=(strategy_combinations.ravel() / 2.0), color=pd_combination_colors)\n",
    "        plt.title('Final Strategy Combinations')\n",
    "        plt.xlabel('Strategy Combination')\n",
    "        plt.ylabel('Number of Agents')\n",
    "        # plt.ylim(0, population)\n",
    "        plt.legend(\n",
    "            [plt.Rectangle((0, 0), 1, 1, color=value) for key, value in strategy_colors.items()],\n",
    "            [strategy_names[key] for key, value in strategy_colors.items()],\n",
    "            loc='upper center',\n",
    "            bbox_to_anchor=(0.5, 1.35),\n",
    "            ncol=5,\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Plot Prisoner's Dilemma Strategy Pairings\n",
    "        for pairing, values in recorded_agent_strategy_pairings.items():\n",
    "            plt.plot(values, '-', color=strategy_colors[pairing[0]], linewidth=3)\n",
    "            plt.plot(values, '--', color=strategy_colors[pairing[1]], linewidth=3)\n",
    "        \n",
    "        plt.title(\"Prisoner's Dilemma Strategy Pairings Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Number of Pairings')\n",
    "        plt.legend(\n",
    "            [plt.Rectangle((0, 0), 1, 1, color=strategy_colors[strategy]) for strategy in StrategyPD],\n",
    "            [strategy_names[strategy] for strategy in StrategyPD],\n",
    "            loc='upper center',\n",
    "            bbox_to_anchor=(0.5, 1.35),\n",
    "            ncol=5,\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Plot Average Partner Selection Q-Values in each episode\n",
    "        plt.plot(agent_qvales_ps[0])\n",
    "        plt.plot(agent_qvales_ps[1])\n",
    "        plt.plot(agent_qvales_ps[2], linestyle='dotted')\n",
    "        plt.plot(agent_qvales_ps[3], linestyle='dotted')\n",
    "        \n",
    "        plt.title(\"Partner Selection Q-Values in each episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Q-Value')\n",
    "        plt.legend([\"Leave Given Parter Previously Defected\", \"Stay Given Parter Previously Defected\", \n",
    "                    \"Leave Given Parter Previously Cooperated\", \"Stay Given Parter Previously Cooperated\"],\n",
    "                    loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Plot Average Prisoner's Dilemma Q-Values in each episode\n",
    "        plt.plot(agent_qvales_pd[0])\n",
    "        plt.plot(agent_qvales_pd[1])\n",
    "        plt.plot(agent_qvales_pd[2], linestyle='dotted')\n",
    "        plt.plot(agent_qvales_pd[3], linestyle='dotted')\n",
    "        \n",
    "        plt.title(\"Prisoner's Dilemma Q-Values in each episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Q-Value')\n",
    "        plt.legend([\"Defect Given Parter Previously Defected\", \"Cooperate Given Parter Previously Defected\", \n",
    "                    \"Defect Given Parter Previously Cooperated\", \"Cooperate Given Parter Previously Cooperated\"],\n",
    "                    loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Agents Prisoner's Dilemma Q-Values in each episode\n",
    "        for idx in range(population):\n",
    "            plt.subplot(211)\n",
    "            plt.plot(recorded_qvalues_ps[idx][0])\n",
    "            plt.plot(recorded_qvalues_ps[idx][1])\n",
    "            plt.plot(recorded_qvalues_ps[idx][2], linestyle='dotted')\n",
    "            plt.plot(recorded_qvalues_ps[idx][3], linestyle='dotted')\n",
    "            \n",
    "            plt.title(\"Agent \" + str(idx) + \" Partner Selection Q-Values in each episode\")\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Q-Value')\n",
    "            plt.legend([\"Leave Given Parter Previously Defected\", \"Stay Given Parter Previously Defected\", \n",
    "                        \"Leave Given Parter Previously Cooperated\", \"Stay Given Parter Previously Cooperated\"],\n",
    "                        loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "            plt.show()\n",
    "\n",
    "            plt.subplot(212)\n",
    "            plt.plot(recorded_qvalues_pd[idx][0])\n",
    "            plt.plot(recorded_qvalues_pd[idx][1])\n",
    "            plt.plot(recorded_qvalues_pd[idx][2], linestyle='dotted')\n",
    "            plt.plot(recorded_qvalues_pd[idx][3], linestyle='dotted')\n",
    "            \n",
    "            plt.title(\"Agent \" + str(idx) + \" Prisoner's Dilemma Q-Values in each episode\")\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Q-Value')\n",
    "            plt.legend([\"Defect Given Parter Previously Defected\", \"Cooperate Given Parter Previously Defected\", \n",
    "                        \"Defect Given Parter Previously Cooperated\", \"Cooperate Given Parter Previously Cooperated\"],\n",
    "                        loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        # plot recorded outcome changes\n",
    "        outcome_changes_legend = []\n",
    "        for keys, changes in recorded_outcome_changes.items():\n",
    "            if keys[0] == (ActionPD.COOPERATE, ActionPD.COOPERATE):\n",
    "                style = 'solid'\n",
    "            elif keys[0] == (ActionPD.COOPERATE, ActionPD.DEFECT):\n",
    "                style = 'dotted'\n",
    "            elif keys[0] == (ActionPD.DEFECT, ActionPD.COOPERATE):\n",
    "                style = 'dashed'\n",
    "            elif keys[0] == (ActionPD.DEFECT, ActionPD.DEFECT):\n",
    "                style = 'dashdot'\n",
    "\n",
    "            if keys[1] == (ActionPD.COOPERATE, ActionPD.COOPERATE):\n",
    "                color = 'red'\n",
    "            elif keys[1] == (ActionPD.COOPERATE, ActionPD.DEFECT):\n",
    "                color = 'green'\n",
    "            elif keys[1] == (ActionPD.DEFECT, ActionPD.COOPERATE):\n",
    "                color = 'orange'\n",
    "            elif keys[1] == (ActionPD.DEFECT, ActionPD.DEFECT):\n",
    "                color = 'blue'\n",
    "            \n",
    "            changes = np.divide(changes, rounds * population)\n",
    "\n",
    "            plt.plot(changes, linestyle=style, color=color)\n",
    "            outcome_changes_legend.append(\"(%s, %s) -> (%s, %s)\" % (keys[0][0].name, keys[0][1].name, keys[1][0].name, keys[1][1].name))\n",
    "        \n",
    "\n",
    "        # Plot the changes in outcomes for prisoner's dilemma games per episode\n",
    "        plt.title(\"Prisoner's Dilemma Outcome Changes Per Episode\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Changes')\n",
    "        plt.legend(outcome_changes_legend, loc='center left', bbox_to_anchor=(1.04, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"agents\": agents,\n",
    "        \"recorded_outcomes_pd\": recorded_outcomes_pd,\n",
    "        \"recorded_agent_strategy_pairings\": recorded_agent_strategy_pairings,\n",
    "        \"recorded_outcome_changes\": recorded_outcome_changes,\n",
    "        \"agent_qvales_ps\": agent_qvales_ps,\n",
    "        \"agent_qvales_pd\": agent_qvales_pd,\n",
    "        \"recorded_qvalues_ps\": recorded_qvalues_ps,\n",
    "        \"recorded_qvalues_pd\": recorded_qvalues_pd,\n",
    "        \"agent_ps_actions_per_episode\": agent_ps_actions_per_episode,\n",
    "        \"percentage_of_states_per_episode\": percentage_of_states_per_episode,\n",
    "        \"agent_chosen_switches_per_episode\": agent_chosen_switches_per_episode,\n",
    "        \"agent_switches_per_episode\": agent_switches_per_episode,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"ps_strategies\": {\n",
    "            StrategyPS.ALWAYS_LEAVE: strat_always_leave,\n",
    "            StrategyPS.OUT_FOR_TAT: strat_out_for_tat,\n",
    "            StrategyPS.REVERSE_OUT_FOR_TAT: strat_reverse_out_for_tat,\n",
    "            StrategyPS.ALWAYS_STAY: strat_always_stay,\n",
    "        },\n",
    "        \"pd_strategies\": {\n",
    "            StrategyPD.ALWAYS_DEFECT: strat_always_defect,\n",
    "            StrategyPD.TIT_FOR_TAT: strat_tit_for_tat,\n",
    "            StrategyPD.REVERSE_TIT_FOR_TAT: strat_reverse_out_for_tat,\n",
    "            StrategyPD.ALWAYS_COOPERATE: strat_always_cooperate,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "The values in the Prisoner's Dilemma Q-Table aproaches the values in the partner selction Q-Table plus one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.24915591 66.11881413]\n",
      " [68.17328622 68.1104981 ]]\n",
      "[[66.64547404 66.17513406]\n",
      " [67.17914018 67.05468777]]\n",
      "Agent 1) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 73.99812513  73.86754113]\n",
      " [ 75.30159881 146.73943892]]\n",
      "[[ 74.42849958  73.82638707]\n",
      " [ 75.03440438 148.2476193 ]]\n",
      "Agent 2) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[65.88188351 65.51827439]\n",
      " [66.82050299 95.20412701]]\n",
      "[[66.67014101 65.78759667]\n",
      " [66.15837865 96.72551992]]\n",
      "Agent 3) PS-Strategy: Always-Stay, PD-Strategy: Tit-For-Tat\n",
      "[[ 72.77329922  72.93209449]\n",
      " [ 73.86797879 145.12577625]]\n",
      "[[ 73.8705038   73.23274003]\n",
      " [ 73.53315272 146.63437033]]\n",
      "Agent 4) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.96505766 65.80717271]\n",
      " [67.7473364  67.88509588]]\n",
      "[[66.37997332 65.69174669]\n",
      " [67.24234145 66.77810422]]\n",
      "Agent 5) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 71.2336236   71.02425057]\n",
      " [ 72.25719708 140.59875773]]\n",
      "[[ 71.78892734  70.91863766]\n",
      " [ 72.20295747 142.10851244]]\n",
      "Agent 6) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[67.28699402 67.08420508]\n",
      " [68.34009294 68.28616112]]\n",
      "[[67.72056857 67.58387423]\n",
      " [68.36020821 67.87834689]]\n",
      "Agent 7) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.19017276 66.04594312]\n",
      " [68.16215057 68.15737165]]\n",
      "[[66.58186491 66.58169142]\n",
      " [66.90898212 66.84702208]]\n",
      "Agent 8) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[65.79488193 65.64712866]\n",
      " [66.37683941 67.18454656]]\n",
      "[[66.80273549 66.13601684]\n",
      " [69.40449133 65.95825601]]\n",
      "Agent 9) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[68.56368986 68.05918259]\n",
      " [69.40338987 96.95757803]]\n",
      "[[69.01761368 68.11327639]\n",
      " [68.8302169  98.47852139]]\n",
      "Agent 10) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 71.34006429  71.2343943 ]\n",
      " [ 72.27138241 140.593238  ]]\n",
      "[[ 72.09216401  71.46130114]\n",
      " [ 71.98473577 142.10299413]]\n",
      "Agent 11) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 73.49055095  73.0153928 ]\n",
      " [ 74.7570203  146.7251698 ]]\n",
      "[[ 73.58293506  72.95234894]\n",
      " [ 74.11016859 148.23335383]]\n",
      "Agent 12) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 71.56624812  71.54332581]\n",
      " [ 73.13352426 145.06405762]]\n",
      "[[ 72.03418337  71.87225706]\n",
      " [ 72.36162475 146.57266753]]\n",
      "Agent 13) PS-Strategy: Always-Leave, PD-Strategy: Reverse-TFT\n",
      "[[66.93268952 66.90449458]\n",
      " [68.48796717 68.38133668]]\n",
      "[[67.39349733 67.42159142]\n",
      " [67.67168639 67.46450042]]\n",
      "Agent 14) PS-Strategy: Always-Stay, PD-Strategy: Always-Defect\n",
      "[[66.13323598 66.25072693]\n",
      " [67.18317154 67.27169468]]\n",
      "[[66.61490229 66.6118532 ]\n",
      " [67.48313491 67.17561566]]\n",
      "Agent 15) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[66.71437349 66.18312612]\n",
      " [67.47290402 67.49963774]]\n",
      "[[67.19995934 66.47132058]\n",
      " [67.56945454 67.08533606]]\n",
      "Agent 16) PS-Strategy: Out-For-Tat, PD-Strategy: Always-Defect\n",
      "[[67.94156978 66.18536744]\n",
      " [67.3621884  68.63364236]]\n",
      "[[68.38201527 66.43898539]\n",
      " [69.04915564 67.16873475]]\n",
      "Agent 17) PS-Strategy: Always-Stay, PD-Strategy: Always-Cooperate\n",
      "[[ 77.27183792  77.31789387]\n",
      " [ 78.00945181 148.46767937]]\n",
      "[[ 77.65408175  77.92431364]\n",
      " [ 78.12526355 149.97541666]]\n",
      "Agent 18) PS-Strategy: Always-Leave, PD-Strategy: Always-Defect\n",
      "[[66.16312525 66.04706523]\n",
      " [67.20142724 67.13515966]]\n",
      "[[66.62921579 66.49634566]\n",
      " [67.4373638  66.74947907]]\n",
      "Agent 19) PS-Strategy: Out-For-Tat, PD-Strategy: Tit-For-Tat\n",
      "[[ 75.639387    75.56344844]\n",
      " [ 76.88949221 148.45336796]]\n",
      "[[ 76.77425124  76.11299535]\n",
      " [ 76.37374148 149.96110892]]\n"
     ]
    }
   ],
   "source": [
    "params = {\"population\": 20,   # Agent Population Size (Must be a multiple of 2)\n",
    "     \"rounds\": 20,            # Rounds per Episode\n",
    "     \"episodes\": 1500,        # Number of Episodes\n",
    "     \"learning_rate\": 0.05,   # Alpha (Learning Rate)\n",
    "     \"temperature\": 85,       # Starting Boltzmann Temperature \n",
    "     \"discount_rate\": 0.99,   # Gamma (Discount Rate)\n",
    "     \"delta_t\": 0.992,        # Boltzmann Temperature Decay Rate\n",
    "     \"disposition\": 0.5,      # Disposition to Assume Cooperation\n",
    "     \"know_fresh_agent\": 0.5, # Probability of Knowing Fresh Agent's Previous Action\n",
    "     \"prefer_same_pool\": 0.0, # Probability of Choosing Same Pool Partner\n",
    "     \"prefer_different_pool\": 0.0, # Probability of Choosing Different Pool Partner\n",
    "     \"learning_mode\": \"q_learning\", # Learning Mode (q_learning or sarsa)\n",
    "     \"do_plot\": True}         # Plot Results\n",
    "\n",
    "results = sdoo(**params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evolution12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
